{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Regression Dataset - Linear Regression vs. XGBoost (Local Mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with XGBoost training locally to notebook\n",
    "XGBoost, or [Extreme Gradient Boosting](https://www.datacamp.com/community/tutorials/xgboost-in-python), is a family of boosting algorithms that uses gradient boosting framework at its core.\n",
    "\n",
    "This section will work with XGBoost as a local installation to the instance.\n",
    "\n",
    "* This will take several minutes to train (even with a small amount of data)\n",
    "* When algorithm supported by *Python*, the data can be locally to the instance\n",
    "* In this section: Compare XBGoost to Linear Regression against dataset\n",
    "\n",
    "**Kernel used:** Conda with TensorFlow Python 3.6.5 for Amazon Elastic Instance *(conda_amazonei_tensorflow_p36)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Library Versions Used\n",
    "\n",
    "| Library | Version |\n",
    "|---------|:--------|\n",
    "| conda | 4.8.2 |\n",
    "| matplotlib | 3.0.3 |\n",
    "| numpy | 1.17.4 |\n",
    "| pandas | 0.24.2 |\n",
    "| pip | 20.2 |\n",
    "| python | 3.6.5 |\n",
    "| xgboost | 0.90 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First update *conda* and *pip* to latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda -y\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure required packages are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list conda\n",
    "!conda list numpy\n",
    "!conda list pandas\n",
    "!conda list pip\n",
    "!conda list python\n",
    "!conda list matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If missing required libraries: uncomment\n",
    "# ! conda install <package from previous step>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install XGboost into the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here: I am using *pip* to install.\n",
    "\n",
    "Ensure that a kernel is running before installing.\n",
    "***Note:*** *This may take several minutes for the initial installation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error # to calculate the regression loss\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('linear_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.x, df.y, label='Target')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.title('Simmple Regression Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'linearTrain.csv'\n",
    "validation_file = 'linearValidation.csv'\n",
    "\n",
    "# Specify the column names, since the files do not have headers\n",
    "df_train = pd.read_csv(train_file, names=['y','x'])\n",
    "df_validation = pd.read_csv(validation_file, names=['y','x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head() # data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_train.x, df_train.y, label='Training', marker='.')\n",
    "plt.scatter(df_validation.x, df_validation.y, label='Validation', marker='.')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Simple Regression Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Features and Targets for Training and Validation\n",
    "This is in preparation for use in XGBoost's regressor\n",
    "*Note: Remember that Python indices start at 0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:,1:] # Features pull from 2nd column to the end\n",
    "y_train = df_train.iloc[:,0].ravel() # Target: 1st Column (0th) Recall: ravel to flatten array\n",
    "\n",
    "X_validation = df_validation.iloc[:,1:]\n",
    "y_validation = df_validation.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the XGBoosst Regressor model\n",
    "\n",
    "Below cells will set up the training instance, set the hyperparameters, and then fit the model to the training data.\n",
    "\n",
    "Find Distributed (Deep) Machine Learning Community's XGBoost Training Parameter Reference [here](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst)\n",
    "\n",
    "In this project, I updated the following parameters:\n",
    "```reg:squarederror``` as ```reg:linear``` is deprecated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regressor\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor # display hyperparameters. Note: This will display at the end of the training model process, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the Training and Validations Datasets\n",
    "# XGBoost will report the training and validation errors\n",
    "# While training, the errors should trend downwards\n",
    "regressor.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_validation, y_validation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model's result, one can see that the training *rmse* trends downward as the model improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Errors\n",
    "First, pull the Training RMSE and Evaluation RMSE values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = regressor.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result # display results in a new format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rounds = range(len(eval_result['validation_0']['rmse'])) # x-axis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Interations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('XGBoost Training vs. Validation Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Feature Importance\n",
    "*plot_importance* function shows which features were usefule in the model's operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(regressor) # To find which features were useful in the model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, *x* was the only feature. This become more interesting with more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset: Compare Actual and Predicted\n",
    "This section focused on evaluating the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=regressor.predict(X_validation) # predicted results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_validation.x, df_validation.y, label='actual', marker='.')\n",
    "plt.scatter(df_validation.x, result, label='predicted', marker='.')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('XGBoost: Validation Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Metrics\n",
    "Calculate the *mean squared error* and *root mean squared error*.\n",
    "Reminder: RMSE is the standard deviation of the residuals (prediction errors), or how well that predicted data concentrated around the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Root Mean Square Error (RMSE) Metrics\n",
    "print('XGBoost Algorithm Metrics')\n",
    "mse = mean_squared_error(df_validation.y,result)\n",
    "print(' Mean Squared Error: {0: .2f}'.format(mse))\n",
    "print(' Root Mean Squared Error: {0: .2f}'.format(mse**.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost Residual Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Residuals\n",
    "residuals = df_validation.y - result\n",
    "plt.hist(residuals)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Actual - Predicted')\n",
    "plt.ylabel('Count')\n",
    "plt.title('XGBoost Residual')\n",
    "plt.axvline(color='r') # overall center of data deviations\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Plot Predicted vs. Actual Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "plt.plot(df.x, df.y, label='Target')\n",
    "plt.plot(df.x, regressor.predict(df[['x']]), label='Predicted')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.title('XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots are nearly identical. This is a great performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Algorithm\n",
    "This section will set up *sklearn's* linear regression for comparison to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor = LinearRegression() # from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Weights Assigned by Linear Regression\n",
    "Using the original function: _5*x + 8 + noise_\n",
    "Below will show the following weights:\n",
    "* coeffecient(s) -> coef_\n",
    "* intercept -> intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor.coef_ # Do not forget underscore '_' at the end. \n",
    "# This will estimate the coefficient for the linear regression plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the array value is very close to the actual coefficient *5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the array value is very close to the actual coefficient *8*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_result = linear_regressor.predict(df_validation[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_validation.x,df_validation.y,label='actual',marker='.')\n",
    "plt.scatter(df_validation.x,linear_result,label='predicted',marker='.')\n",
    "plt.grid(True)\n",
    "plt.title('LinearRegression - Validation Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows where the line expected to be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Metrics\n",
    "Calculate the *mean squared error* and *root mean squared error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Root Mean Square Error (RMSE) Metrics\n",
    "print('Linear Regression Algorithm Metrics')\n",
    "mse = mean_squared_error(df_validation.y,linear_result)\n",
    "print(' Mean Squared Error: {0: .2f}'.format(mse))\n",
    "print(' Root Mean Squared Error: {0: .2f}'.format(mse**.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Residual Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Residuals\n",
    "residuals = df_validation.y - linear_result\n",
    "plt.hist(residuals)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Actual - Predicted')\n",
    "plt.ylabel('Count')\n",
    "plt.title('XGBoost Residual')\n",
    "plt.axvline(color='r') # overall center of data deviations\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression in this case performed better than XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Plot Predicted vs. Actual Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset\n",
    "plt.plot(df.x, df.y, label='Target')\n",
    "plt.plot(df.x, linear_regressor.predict(df[['x']]), label='Predicted')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.title('XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Features - Outside of Range Used for Training\n",
    "* XGBoost Prediction has an upper and lower bound (directly applies to tree-based algorithms)\n",
    "* Linear Regression extrapolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revist the function\n",
    "def straight_line(x):\n",
    "    return 5*x+8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X is outside the training samples' range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-100,-5,0.5,1,1.9,5,29,49,160,1000,5000])\n",
    "y = straight_line(X)\n",
    "\n",
    "df_IF = pd.DataFrame({'x':X,'y':y})\n",
    "df_IF['xgboost']=regressor.predict(df_IF[['x']])\n",
    "df_IF['linear']=linear_regressor.predict(df_IF[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IF # display values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost have caps for upper and lower bounds, not reach the extent of *y*, due to being designed to be memory efficient. The *upper* bound is set to *X=149* and *lower* bound to *X=1*, as default, due to regressor being configured to ```reg:squarederror```.\n",
    "* Linear followed the *y* values nearly identically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Outside of Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Predictions: upper and lower bounds\n",
    "# Linear Regression: extrapolation\n",
    "plt.scatter(df_IF.x, df_IF.y, label='Actual',color='red')\n",
    "plt.plot(df_IF.x,df_IF.linear,label='LinearRegression')\n",
    "plt.plot(df_IF.x,df_IF.xgboost,label='XGBoost')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Input Outside Range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the straight horizontal shows where XGBoost bounded.\n",
    "\n",
    "Red dots are the 'ground truth.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X is Inside the training samples' range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1,3,5,7,89,110,125,149]) # Values changed\n",
    "y = straight_line(X)\n",
    "\n",
    "df_IF = pd.DataFrame({'x':X,'y':y})\n",
    "df_IF['xgboost']=regressor.predict(df_IF[['x']])\n",
    "df_IF['linear']=linear_regressor.predict(df_IF[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IF # display values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost and Linear followed the *y* values closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Inside of Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Predictions: upper and lower bounds\n",
    "# Linear Regression: extrapolation\n",
    "plt.scatter(df_IF.x, df_IF.y, label='Actual',color='red')\n",
    "plt.plot(df_IF.x,df_IF.linear,label='LinearRegression')\n",
    "plt.plot(df_IF.x,df_IF.xgboost,label='XGBoost')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Input Inside Range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, there is no straight horizontal for XGBoost bound, due to inputs inside being in its range.\n",
    "\n",
    "Red dots are the 'ground truth.'\n",
    "\n",
    "The reason for XGBoost's bounding is to concentrate on decision tree-like operations. Many times, the branches do not need wide array of values to make the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. Updated core installation tools\n",
    "1. Checked for required libraries\n",
    "1. Installed `xgboost` for local mode\n",
    "1. Built training and validation datasets\n",
    "1. Built `xgboost` Regressor\n",
    "1. Built *Linear Regression* care of `sklearn`\n",
    "1. Explored performance 'Out of Range' and 'In Range' Inputs with *XGBoost* and *Linear Regression*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
