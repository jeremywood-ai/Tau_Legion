{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package versions used in this project\n",
    "\n",
    "| **Package** | Version |\n",
    "|-------------|---------|\n",
    "| numpy | 1.14.6 |\n",
    "| numy-base | 1.14.6 |\n",
    "| pandas | 0.24.2 |\n",
    "| jsonschema | 2.6.0 |\n",
    "| matplotlib | 3.1.3 |\n",
    "| matplotlib-base | 3.1.3 |\n",
    "| boto3 | 1.12.0 |\n",
    "| sagemaker | 1.55.4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Base Name Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a good base job name when building different models\n",
    "# This helps with identifying training models and endpoints\n",
    "with_categories = False\n",
    "if with_categories:\n",
    "    base_job_name = 'deepar-biketrain-with-categories'\n",
    "else:\n",
    "    base_job_name = 'deepar-biketrain-no-categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Your Bucket Name\n",
    "bucket = 's3-2-ml-sagemaker'\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "if with_categories:\n",
    "    s3_data_path = \"{}/{}/data_with_categories\".format(bucket, prefix)\n",
    "else:\n",
    "    s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "\n",
    "s3_output_path = \"{}/{}/ouput\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3-2-ml-sagemaker/deepar/bikerental/data',\n",
       " 's3-2-ml-sagemaker/deepar/bikerental/ouput')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path, s3_output_path # Review the directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Output File Through Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename, 'rb') as f: # read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training and Test Files to S3 bucket\n",
    "\n",
    "Using JSON files created in the data preparation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_categories:\n",
    "    write_to_s3('train_with_categories.json',bucket,'deepar/bikerental/data_with_categories/train/train_with_categories.json')\n",
    "    write_to_s3('test_with_categories.json',bucket,'deepar/bikerental/data_with_categories/test/test_with_categories.json')\n",
    "else:\n",
    "    write_to_s3('train.json',bucket,'deepar/bikerental/data/train/train.json')\n",
    "    write_to_s3('test.json',bucket,'deepar/bikerental/data/test/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SageMaker's Estimator\n",
    "Ref: [SageMaker - Amazon Estimators 1.55.4](https://sagemaker.readthedocs.io/en/stable/sagemaker.amazon.amazon_estimator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name # data review - verify Docker image for the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Billing Tiers\n",
    "# https://aws.amazon.com/sagemaker/pricing/\n",
    "# If you are outside of free-tier, you can also use ml.m5.xlarge  (newer generation instance)\n",
    "# In this example, I am using ml.m5.xlarge for training\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 288, 288)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters\n",
    "\n",
    "Ref: [SageMaker DeepAr Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)\n",
    "\n",
    "Notes: Adjusted Hyperparameters\n",
    "1. time frequency set to H (freq)\n",
    "1. epochs 400\n",
    "1. early stopping patience 40, stop after X epochs, if no improvement\n",
    "1. mini-batch set to 128\n",
    "1. learning rate 5E-4, 0.0005\n",
    "1. context length set to 288 (string of context_length)\n",
    "1. prediction length set to 288 (string of prediction_length)\n",
    "1. cardinality : auto for data tht has categories or *nothing*. This is an array specifying the number of categories (groups) per categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'H',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '40',\n",
       " 'mini_batch_size': '128',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '288',\n",
       " 'prediction_length': '288',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters # verify the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Referenced Training and Testing Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker will use all the files available to it, in JSON format\n",
    "data_channel = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-25 03:26:35 Starting - Starting the training job...\n",
      "2020-04-25 03:26:37 Starting - Launching requested ML instances...\n",
      "2020-04-25 03:27:33 Starting - Preparing the instances for training......\n",
      "2020-04-25 03:28:23 Downloading - Downloading input data\n",
      "2020-04-25 03:28:23 Training - Downloading the training image.....\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'288', u'epochs': u'400', u'time_freq': u'H', u'context_length': u'288', u'mini_batch_size': u'128', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'288', u'time_freq': u'H', u'context_length': u'288', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Integer time series\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] number of time series: 3\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] mean target length: 16968\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] min/mean/max target: 0.0/79.5729608675/977.0\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] mean abs(target): 79.5729608675\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Small number of time series. Doing 427 passes over dataset with prob 0.999219359875 per epoch.\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Integer time series\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] number of time series: 3\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] number of observations: 51768\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] mean target length: 17256\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] min/mean/max target: 0.0/80.5700819039/977.0\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] mean abs(target): 80.5700819039\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] contains missing values: yes (36.9%)\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] nvidia-smi took: 0.0251228809357 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:17 INFO 140327940233024] Create Store: local\u001b[0m\n",
      "\n",
      "2020-04-25 03:29:15 Training - Training image download completed. Training in progress.\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 24275.9051322937, \"sum\": 24275.9051322937, \"min\": 24275.9051322937}}, \"EndTime\": 1587785382.081738, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785357.805045}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:42 INFO 140327940233024] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 27109.374046325684, \"sum\": 27109.374046325684, \"min\": 27109.374046325684}}, \"EndTime\": 1587785384.914533, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785382.082143}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:56 INFO 140327940233024] Epoch[0] Batch[0] avg_epoch_loss=4.022502\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:29:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=4.0225019455\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:06 INFO 140327940233024] Epoch[0] Batch[5] avg_epoch_loss=3.951944\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.95194411278\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:06 INFO 140327940233024] Epoch[0] Batch [5]#011Speed: 65.52 samples/sec#011loss=3.951944\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:14 INFO 140327940233024] processed a total of 1225 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 29100.309133529663, \"sum\": 29100.309133529663, \"min\": 29100.309133529663}}, \"EndTime\": 1587785414.015061, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785384.914665}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:14 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.0955992627 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:14 INFO 140327940233024] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.83843340874\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:14 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:14 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_3c8ee07e-0ac9-4e40-9cfa-c4794a63edad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 201.3721466064453, \"sum\": 201.3721466064453, \"min\": 201.3721466064453}}, \"EndTime\": 1587785414.217084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785414.015146}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:25 INFO 140327940233024] Epoch[1] Batch[0] avg_epoch_loss=3.654306\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.65430593491\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:34 INFO 140327940233024] Epoch[1] Batch[5] avg_epoch_loss=3.620318\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.62031805515\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:34 INFO 140327940233024] Epoch[1] Batch [5]#011Speed: 69.57 samples/sec#011loss=3.620318\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] Epoch[1] Batch[10] avg_epoch_loss=3.704903\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=3.80640387535\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] Epoch[1] Batch [10]#011Speed: 69.47 samples/sec#011loss=3.806404\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29668.59006881714, \"sum\": 29668.59006881714, \"min\": 29668.59006881714}}, \"EndTime\": 1587785443.885799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785414.217149}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.0194697335 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.70490251888\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:43 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:44 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_0713af7c-ca38-41e7-b952-ea9ee98b3224-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 199.71394538879395, \"sum\": 199.71394538879395, \"min\": 199.71394538879395}}, \"EndTime\": 1587785444.085985, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785443.885867}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:55 INFO 140327940233024] Epoch[2] Batch[0] avg_epoch_loss=3.629026\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:30:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.62902617455\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:04 INFO 140327940233024] Epoch[2] Batch[5] avg_epoch_loss=3.514728\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.51472822825\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:04 INFO 140327940233024] Epoch[2] Batch [5]#011Speed: 69.39 samples/sec#011loss=3.514728\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:11 INFO 140327940233024] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27747.963905334473, \"sum\": 27747.963905334473, \"min\": 27747.963905334473}}, \"EndTime\": 1587785471.834072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785444.086047}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:11 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.9131212898 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:11 INFO 140327940233024] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.49352440834\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:11 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:12 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_ace917ce-78e9-4e81-98c6-49b9871a4f1c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 197.11685180664062, \"sum\": 197.11685180664062, \"min\": 197.11685180664062}}, \"EndTime\": 1587785472.031823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785471.834134}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:22 INFO 140327940233024] Epoch[3] Batch[0] avg_epoch_loss=3.527984\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.52798366547\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:32 INFO 140327940233024] Epoch[3] Batch[5] avg_epoch_loss=3.429980\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.42997976144\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:32 INFO 140327940233024] Epoch[3] Batch [5]#011Speed: 69.64 samples/sec#011loss=3.429980\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:39 INFO 140327940233024] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27598.657846450806, \"sum\": 27598.657846450806, \"min\": 27598.657846450806}}, \"EndTime\": 1587785499.630589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785472.03188}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.4368371081 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.41441032887\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:39 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:39 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_d6e6045b-c63e-4007-aeda-fef351d9128c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 197.6330280303955, \"sum\": 197.6330280303955, \"min\": 197.6330280303955}}, \"EndTime\": 1587785499.828794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785499.630656}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:50 INFO 140327940233024] Epoch[4] Batch[0] avg_epoch_loss=3.326070\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:31:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.32606959343\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:00 INFO 140327940233024] Epoch[4] Batch[5] avg_epoch_loss=3.351534\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.35153369109\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:00 INFO 140327940233024] Epoch[4] Batch [5]#011Speed: 68.32 samples/sec#011loss=3.351534\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] Epoch[4] Batch[10] avg_epoch_loss=3.191381\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=2.99919776917\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] Epoch[4] Batch [10]#011Speed: 67.80 samples/sec#011loss=2.999198\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29870.72706222534, \"sum\": 29870.72706222534, \"min\": 29870.72706222534}}, \"EndTime\": 1587785529.699632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785499.828852}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2572326156 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.19138099931\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:09 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_2e48f337-683b-4cf0-9020-b394c2450e43-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 198.1801986694336, \"sum\": 198.1801986694336, \"min\": 198.1801986694336}}, \"EndTime\": 1587785529.898384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785529.6997}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:21 INFO 140327940233024] Epoch[5] Batch[0] avg_epoch_loss=3.392672\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.39267206192\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:30 INFO 140327940233024] Epoch[5] Batch[5] avg_epoch_loss=3.283267\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.28326662381\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:30 INFO 140327940233024] Epoch[5] Batch [5]#011Speed: 68.21 samples/sec#011loss=3.283267\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] Epoch[5] Batch[10] avg_epoch_loss=3.198546\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=3.09688138962\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] Epoch[5] Batch [10]#011Speed: 67.81 samples/sec#011loss=3.096881\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29981.57501220703, \"sum\": 29981.57501220703, \"min\": 29981.57501220703}}, \"EndTime\": 1587785559.880082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785529.898447}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.0262649094 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.19854606282\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:39 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:51 INFO 140327940233024] Epoch[6] Batch[0] avg_epoch_loss=3.218052\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:32:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.21805214882\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:00 INFO 140327940233024] Epoch[6] Batch[5] avg_epoch_loss=3.185494\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.18549358845\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:00 INFO 140327940233024] Epoch[6] Batch [5]#011Speed: 68.89 samples/sec#011loss=3.185494\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:07 INFO 140327940233024] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28051.229000091553, \"sum\": 28051.229000091553, \"min\": 28051.229000091553}}, \"EndTime\": 1587785587.931837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785559.880158}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:07 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.0958766876 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:07 INFO 140327940233024] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:07 INFO 140327940233024] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.16249499321\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:07 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:08 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_df84ae6a-7009-497e-a7d2-a0d2ecd3ef18-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 221.05789184570312, \"sum\": 221.05789184570312, \"min\": 221.05789184570312}}, \"EndTime\": 1587785588.153492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785587.931915}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:19 INFO 140327940233024] Epoch[7] Batch[0] avg_epoch_loss=3.058837\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:19 INFO 140327940233024] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.05883717537\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:28 INFO 140327940233024] Epoch[7] Batch[5] avg_epoch_loss=3.108430\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.1084296306\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:28 INFO 140327940233024] Epoch[7] Batch [5]#011Speed: 68.63 samples/sec#011loss=3.108430\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:36 INFO 140327940233024] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28070.29700279236, \"sum\": 28070.29700279236, \"min\": 28070.29700279236}}, \"EndTime\": 1587785616.223942, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785588.153559}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:36 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1008539512 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:36 INFO 140327940233024] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.08100428581\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:36 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:36 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_585bca04-191f-44cc-abb1-4cdc3c6c570a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 198.3499526977539, \"sum\": 198.3499526977539, \"min\": 198.3499526977539}}, \"EndTime\": 1587785616.422886, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785616.22401}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:47 INFO 140327940233024] Epoch[8] Batch[0] avg_epoch_loss=3.140019\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:47 INFO 140327940233024] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=3.14001870155\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:56 INFO 140327940233024] Epoch[8] Batch[5] avg_epoch_loss=3.049260\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.04925958316\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:33:56 INFO 140327940233024] Epoch[8] Batch [5]#011Speed: 69.24 samples/sec#011loss=3.049260\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:04 INFO 140327940233024] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27818.25089454651, \"sum\": 27818.25089454651, \"min\": 27818.25089454651}}, \"EndTime\": 1587785644.241266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785616.422951}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:04 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8624756695 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:04 INFO 140327940233024] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.04835181236\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:04 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:04 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_52073877-82f9-40ed-84f7-f88cee8d50f8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 203.7498950958252, \"sum\": 203.7498950958252, \"min\": 203.7498950958252}}, \"EndTime\": 1587785644.445505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785644.241329}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:15 INFO 140327940233024] Epoch[9] Batch[0] avg_epoch_loss=3.115251\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.11525082588\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:24 INFO 140327940233024] Epoch[9] Batch[5] avg_epoch_loss=3.035380\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.03537984689\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:24 INFO 140327940233024] Epoch[9] Batch [5]#011Speed: 69.82 samples/sec#011loss=3.035380\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:32 INFO 140327940233024] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27563.796043395996, \"sum\": 27563.796043395996, \"min\": 27563.796043395996}}, \"EndTime\": 1587785672.009424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785644.445563}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:32 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.5305541687 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:32 INFO 140327940233024] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.0102889061\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:32 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:32 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_b61628a6-9965-4cf5-941b-c5ed7536ca70-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 194.91887092590332, \"sum\": 194.91887092590332, \"min\": 194.91887092590332}}, \"EndTime\": 1587785672.205013, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785672.009497}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:43 INFO 140327940233024] Epoch[10] Batch[0] avg_epoch_loss=2.954558\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.95455789566\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:52 INFO 140327940233024] Epoch[10] Batch[5] avg_epoch_loss=2.979384\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.97938366731\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:34:52 INFO 140327940233024] Epoch[10] Batch [5]#011Speed: 69.65 samples/sec#011loss=2.979384\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] Epoch[10] Batch[10] avg_epoch_loss=2.985045\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=2.99183964729\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] Epoch[10] Batch [10]#011Speed: 69.18 samples/sec#011loss=2.991840\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29630.90991973877, \"sum\": 29630.90991973877, \"min\": 29630.90991973877}}, \"EndTime\": 1587785701.836055, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785672.205081}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2317257727 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.98504547639\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:01 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:02 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_de4f4a17-227f-4c68-9cfa-d60f1283767b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 199.74398612976074, \"sum\": 199.74398612976074, \"min\": 199.74398612976074}}, \"EndTime\": 1587785702.036354, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785701.836127}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:13 INFO 140327940233024] Epoch[11] Batch[0] avg_epoch_loss=2.902776\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.90277576447\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:22 INFO 140327940233024] Epoch[11] Batch[5] avg_epoch_loss=2.896015\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.89601480961\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:22 INFO 140327940233024] Epoch[11] Batch [5]#011Speed: 68.10 samples/sec#011loss=2.896015\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] Epoch[11] Batch[10] avg_epoch_loss=2.976894\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=3.07394843102\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] Epoch[11] Batch [10]#011Speed: 67.68 samples/sec#011loss=3.073948\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30143.254041671753, \"sum\": 30143.254041671753, \"min\": 30143.254041671753}}, \"EndTime\": 1587785732.179733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785702.03641}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.0892722675 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.97689372843\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:32 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_78ab94f4-6bd2-4ab4-a5c6-cedc16b3f068-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 202.36802101135254, \"sum\": 202.36802101135254, \"min\": 202.36802101135254}}, \"EndTime\": 1587785732.382657, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785732.179819}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:43 INFO 140327940233024] Epoch[12] Batch[0] avg_epoch_loss=2.893202\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.89320206642\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:52 INFO 140327940233024] Epoch[12] Batch[5] avg_epoch_loss=2.905838\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.90583841006\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:52 INFO 140327940233024] Epoch[12] Batch [5]#011Speed: 69.55 samples/sec#011loss=2.905838\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:59 INFO 140327940233024] processed a total of 1191 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27573.163986206055, \"sum\": 27573.163986206055, \"min\": 27573.163986206055}}, \"EndTime\": 1587785759.955938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785732.382717}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:59 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.1940189386 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:59 INFO 140327940233024] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.85978116989\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:35:59 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:00 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_00cbe6da-15f1-4fe1-91ba-f89a5601fb30-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 221.27008438110352, \"sum\": 221.27008438110352, \"min\": 221.27008438110352}}, \"EndTime\": 1587785760.17784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785759.956004}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:11 INFO 140327940233024] Epoch[13] Batch[0] avg_epoch_loss=3.011618\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.01161789894\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:20 INFO 140327940233024] Epoch[13] Batch[5] avg_epoch_loss=2.929654\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.92965424061\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:20 INFO 140327940233024] Epoch[13] Batch [5]#011Speed: 68.38 samples/sec#011loss=2.929654\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:28 INFO 140327940233024] processed a total of 1181 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27918.129920959473, \"sum\": 27918.129920959473, \"min\": 27918.129920959473}}, \"EndTime\": 1587785788.096096, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785760.177899}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:28 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.302087655 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:28 INFO 140327940233024] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.89678754807\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:28 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:39 INFO 140327940233024] Epoch[14] Batch[0] avg_epoch_loss=2.861570\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.86157011986\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:48 INFO 140327940233024] Epoch[14] Batch[5] avg_epoch_loss=2.945567\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.94556689262\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:48 INFO 140327940233024] Epoch[14] Batch [5]#011Speed: 69.91 samples/sec#011loss=2.945567\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] Epoch[14] Batch[10] avg_epoch_loss=3.011546\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=3.09072051048\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] Epoch[14] Batch [10]#011Speed: 69.66 samples/sec#011loss=3.090721\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29650.313138961792, \"sum\": 29650.313138961792, \"min\": 29650.313138961792}}, \"EndTime\": 1587785817.747, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785788.09617}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4513157779 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.01154580983\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:36:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:09 INFO 140327940233024] Epoch[15] Batch[0] avg_epoch_loss=2.761974\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.76197361946\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:18 INFO 140327940233024] Epoch[15] Batch[5] avg_epoch_loss=2.823070\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.8230701685\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:18 INFO 140327940233024] Epoch[15] Batch [5]#011Speed: 68.64 samples/sec#011loss=2.823070\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] Epoch[15] Batch[10] avg_epoch_loss=2.903752\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=3.00057086945\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] Epoch[15] Batch [10]#011Speed: 66.89 samples/sec#011loss=3.000571\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] processed a total of 1341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30184.251070022583, \"sum\": 30184.251070022583, \"min\": 30184.251070022583}}, \"EndTime\": 1587785847.931732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785817.747069}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4269620497 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.90375230529\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:27 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:39 INFO 140327940233024] Epoch[16] Batch[0] avg_epoch_loss=2.906680\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=2.90667963028\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:48 INFO 140327940233024] Epoch[16] Batch[5] avg_epoch_loss=2.834560\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.83455952009\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:48 INFO 140327940233024] Epoch[16] Batch [5]#011Speed: 69.76 samples/sec#011loss=2.834560\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] Epoch[16] Batch[10] avg_epoch_loss=2.870429\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=2.91347146034\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] Epoch[16] Batch [10]#011Speed: 69.29 samples/sec#011loss=2.913471\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] processed a total of 1336 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29612.715005874634, \"sum\": 29612.715005874634, \"min\": 29612.715005874634}}, \"EndTime\": 1587785877.544839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785847.931821}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1156102347 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.87042858384\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:37:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:08 INFO 140327940233024] Epoch[17] Batch[0] avg_epoch_loss=2.847147\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.8471467495\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:17 INFO 140327940233024] Epoch[17] Batch[5] avg_epoch_loss=2.849695\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.84969536463\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:17 INFO 140327940233024] Epoch[17] Batch [5]#011Speed: 69.85 samples/sec#011loss=2.849695\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:25 INFO 140327940233024] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27824.50294494629, \"sum\": 27824.50294494629, \"min\": 27824.50294494629}}, \"EndTime\": 1587785905.369775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785877.544902}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:25 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.247730456 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:25 INFO 140327940233024] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.83609850407\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:25 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:25 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_2a9ff554-35c4-45db-823a-b1f53e4ad1f9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 200.4261016845703, \"sum\": 200.4261016845703, \"min\": 200.4261016845703}}, \"EndTime\": 1587785905.570714, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785905.369836}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:36 INFO 140327940233024] Epoch[18] Batch[0] avg_epoch_loss=2.858037\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.85803747177\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:45 INFO 140327940233024] Epoch[18] Batch[5] avg_epoch_loss=2.767174\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.76717436314\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:45 INFO 140327940233024] Epoch[18] Batch [5]#011Speed: 69.01 samples/sec#011loss=2.767174\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] Epoch[18] Batch[10] avg_epoch_loss=2.848589\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=2.946286726\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] Epoch[18] Batch [10]#011Speed: 68.77 samples/sec#011loss=2.946287\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29631.4959526062, \"sum\": 29631.4959526062, \"min\": 29631.4959526062}}, \"EndTime\": 1587785935.202333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785905.570776}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.074578051 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.84858907353\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:38:55 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:06 INFO 140327940233024] Epoch[19] Batch[0] avg_epoch_loss=2.785188\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.78518819809\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:15 INFO 140327940233024] Epoch[19] Batch[5] avg_epoch_loss=2.806571\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.80657096704\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:15 INFO 140327940233024] Epoch[19] Batch [5]#011Speed: 70.10 samples/sec#011loss=2.806571\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] Epoch[19] Batch[10] avg_epoch_loss=2.798684\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=2.78921980858\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] Epoch[19] Batch [10]#011Speed: 69.81 samples/sec#011loss=2.789220\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29443.562984466553, \"sum\": 29443.562984466553, \"min\": 29443.562984466553}}, \"EndTime\": 1587785964.646349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785935.202396}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.8124968238 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.79868407683\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:24 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_e8709cde-3548-4f5e-8db6-ccec8c12ac69-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 199.49102401733398, \"sum\": 199.49102401733398, \"min\": 199.49102401733398}}, \"EndTime\": 1587785964.846299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785964.646409}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:36 INFO 140327940233024] Epoch[20] Batch[0] avg_epoch_loss=2.910151\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.91015052795\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:45 INFO 140327940233024] Epoch[20] Batch[5] avg_epoch_loss=2.800870\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.80086994171\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:45 INFO 140327940233024] Epoch[20] Batch [5]#011Speed: 68.46 samples/sec#011loss=2.800870\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] Epoch[20] Batch[10] avg_epoch_loss=2.813496\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=2.82864718437\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] Epoch[20] Batch [10]#011Speed: 68.25 samples/sec#011loss=2.828647\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29981.783866882324, \"sum\": 29981.783866882324, \"min\": 29981.783866882324}}, \"EndTime\": 1587785994.828198, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785964.846357}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2934185579 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.8134959611\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:39:54 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:06 INFO 140327940233024] Epoch[21] Batch[0] avg_epoch_loss=2.789573\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.78957271576\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:15 INFO 140327940233024] Epoch[21] Batch[5] avg_epoch_loss=2.797865\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.79786479473\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:15 INFO 140327940233024] Epoch[21] Batch [5]#011Speed: 68.51 samples/sec#011loss=2.797865\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] Epoch[21] Batch[10] avg_epoch_loss=2.806701\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=2.8173034668\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] Epoch[21] Batch [10]#011Speed: 67.97 samples/sec#011loss=2.817303\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] processed a total of 1344 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30226.79591178894, \"sum\": 30226.79591178894, \"min\": 30226.79591178894}}, \"EndTime\": 1587786025.055451, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587785994.828264}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4637137366 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.80670055476\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:25 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:36 INFO 140327940233024] Epoch[22] Batch[0] avg_epoch_loss=2.700865\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.70086455345\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:45 INFO 140327940233024] Epoch[22] Batch[5] avg_epoch_loss=2.761006\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.76100607713\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:45 INFO 140327940233024] Epoch[22] Batch [5]#011Speed: 69.81 samples/sec#011loss=2.761006\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:52 INFO 140327940233024] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27629.168033599854, \"sum\": 27629.168033599854, \"min\": 27629.168033599854}}, \"EndTime\": 1587786052.685086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786025.055517}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:52 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6988824592 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:52 INFO 140327940233024] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.77545249462\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:52 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:40:52 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_e3cb7a24-1d2e-4d29-91b6-ad73ab82f590-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 205.57403564453125, \"sum\": 205.57403564453125, \"min\": 205.57403564453125}}, \"EndTime\": 1587786052.891207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786052.685209}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:04 INFO 140327940233024] Epoch[23] Batch[0] avg_epoch_loss=2.710779\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.71077942848\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:13 INFO 140327940233024] Epoch[23] Batch[5] avg_epoch_loss=2.763180\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.76317981879\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:13 INFO 140327940233024] Epoch[23] Batch [5]#011Speed: 68.20 samples/sec#011loss=2.763180\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] Epoch[23] Batch[10] avg_epoch_loss=2.661441\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=2.53935418129\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] Epoch[23] Batch [10]#011Speed: 68.09 samples/sec#011loss=2.539354\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30125.707149505615, \"sum\": 30125.707149505615, \"min\": 30125.707149505615}}, \"EndTime\": 1587786083.017025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786052.891263}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.5880781015 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.66144089265\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:23 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_3a438eb9-facc-4685-b45d-c21ea6827742-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 206.82191848754883, \"sum\": 206.82191848754883, \"min\": 206.82191848754883}}, \"EndTime\": 1587786083.22427, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786083.017091}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:34 INFO 140327940233024] Epoch[24] Batch[0] avg_epoch_loss=2.800648\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.80064821243\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:43 INFO 140327940233024] Epoch[24] Batch[5] avg_epoch_loss=2.793110\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.79310961564\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:43 INFO 140327940233024] Epoch[24] Batch [5]#011Speed: 69.85 samples/sec#011loss=2.793110\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] Epoch[24] Batch[10] avg_epoch_loss=2.818373\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=2.8486890316\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] Epoch[24] Batch [10]#011Speed: 69.85 samples/sec#011loss=2.848689\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29480.72099685669, \"sum\": 29480.72099685669, \"min\": 29480.72099685669}}, \"EndTime\": 1587786112.705103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786083.224327}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2999861652 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.81837298653\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:41:52 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:03 INFO 140327940233024] Epoch[25] Batch[0] avg_epoch_loss=2.671510\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:03 INFO 140327940233024] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.67150974274\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:13 INFO 140327940233024] Epoch[25] Batch[5] avg_epoch_loss=2.759974\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.75997364521\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:13 INFO 140327940233024] Epoch[25] Batch [5]#011Speed: 69.74 samples/sec#011loss=2.759974\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:20 INFO 140327940233024] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27752.99596786499, \"sum\": 27752.99596786499, \"min\": 27752.99596786499}}, \"EndTime\": 1587786140.458583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786112.70517}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:20 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.2922260844 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:20 INFO 140327940233024] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.75349502563\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:20 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:31 INFO 140327940233024] Epoch[26] Batch[0] avg_epoch_loss=2.796041\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.79604077339\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:41 INFO 140327940233024] Epoch[26] Batch[5] avg_epoch_loss=2.772200\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.7722004652\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:41 INFO 140327940233024] Epoch[26] Batch [5]#011Speed: 67.83 samples/sec#011loss=2.772200\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] Epoch[26] Batch[10] avg_epoch_loss=2.730892\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.68132090569\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] Epoch[26] Batch [10]#011Speed: 68.42 samples/sec#011loss=2.681321\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30020.246028900146, \"sum\": 30020.246028900146, \"min\": 30020.246028900146}}, \"EndTime\": 1587786170.479349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786140.458657}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2373387427 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.73089157451\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:42:50 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:01 INFO 140327940233024] Epoch[27] Batch[0] avg_epoch_loss=2.627755\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.62775540352\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:11 INFO 140327940233024] Epoch[27] Batch[5] avg_epoch_loss=2.721462\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.72146173318\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:11 INFO 140327940233024] Epoch[27] Batch [5]#011Speed: 67.53 samples/sec#011loss=2.721462\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:18 INFO 140327940233024] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28177.47712135315, \"sum\": 28177.47712135315, \"min\": 28177.47712135315}}, \"EndTime\": 1587786198.657339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786170.479416}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:18 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6517305651 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:18 INFO 140327940233024] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.72157204151\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:18 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:29 INFO 140327940233024] Epoch[28] Batch[0] avg_epoch_loss=2.683180\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.68317985535\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:39 INFO 140327940233024] Epoch[28] Batch[5] avg_epoch_loss=2.752828\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.75282760461\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:39 INFO 140327940233024] Epoch[28] Batch [5]#011Speed: 69.96 samples/sec#011loss=2.752828\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:46 INFO 140327940233024] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27744.559049606323, \"sum\": 27744.559049606323, \"min\": 27744.559049606323}}, \"EndTime\": 1587786226.402382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786198.657408}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:46 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.7745823094 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:46 INFO 140327940233024] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.71822984219\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:46 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:57 INFO 140327940233024] Epoch[29] Batch[0] avg_epoch_loss=2.738430\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:43:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.73843002319\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:06 INFO 140327940233024] Epoch[29] Batch[5] avg_epoch_loss=2.724511\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.72451082865\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:06 INFO 140327940233024] Epoch[29] Batch [5]#011Speed: 68.40 samples/sec#011loss=2.724511\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] Epoch[29] Batch[10] avg_epoch_loss=2.697811\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=2.66577191353\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] Epoch[29] Batch [10]#011Speed: 68.57 samples/sec#011loss=2.665772\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29913.903951644897, \"sum\": 29913.903951644897, \"min\": 29913.903951644897}}, \"EndTime\": 1587786256.316891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786226.402445}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.4579025886 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.69781132178\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:16 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:27 INFO 140327940233024] Epoch[30] Batch[0] avg_epoch_loss=2.742570\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.74257016182\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:36 INFO 140327940233024] Epoch[30] Batch[5] avg_epoch_loss=2.723368\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.72336788972\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:36 INFO 140327940233024] Epoch[30] Batch [5]#011Speed: 68.83 samples/sec#011loss=2.723368\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] Epoch[30] Batch[10] avg_epoch_loss=2.826462\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=2.9501742363\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] Epoch[30] Batch [10]#011Speed: 69.32 samples/sec#011loss=2.950174\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29787.076950073242, \"sum\": 29787.076950073242, \"min\": 29787.076950073242}}, \"EndTime\": 1587786286.104462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786256.316961}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.8107864852 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.82646168362\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:46 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:57 INFO 140327940233024] Epoch[31] Batch[0] avg_epoch_loss=2.687927\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:44:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.68792700768\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:06 INFO 140327940233024] Epoch[31] Batch[5] avg_epoch_loss=2.733919\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.7339190642\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:06 INFO 140327940233024] Epoch[31] Batch [5]#011Speed: 67.60 samples/sec#011loss=2.733919\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] Epoch[31] Batch[10] avg_epoch_loss=2.674406\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=2.60299005508\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] Epoch[31] Batch [10]#011Speed: 67.70 samples/sec#011loss=2.602990\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30074.037075042725, \"sum\": 30074.037075042725, \"min\": 30074.037075042725}}, \"EndTime\": 1587786316.17897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786286.104536}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.7277458593 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.67440587824\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:16 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:27 INFO 140327940233024] Epoch[32] Batch[0] avg_epoch_loss=2.783192\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.78319191933\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:36 INFO 140327940233024] Epoch[32] Batch[5] avg_epoch_loss=2.697155\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.69715531667\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:36 INFO 140327940233024] Epoch[32] Batch [5]#011Speed: 68.76 samples/sec#011loss=2.697155\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] Epoch[32] Batch[10] avg_epoch_loss=2.595582\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=2.47369332314\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] Epoch[32] Batch [10]#011Speed: 68.38 samples/sec#011loss=2.473693\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29838.128089904785, \"sum\": 29838.128089904785, \"min\": 29838.128089904785}}, \"EndTime\": 1587786346.017558, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786316.179037}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2666471707 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.59558168325\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:46 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_cc313a3c-f424-4200-b696-613c53e7bcf8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 210.6950283050537, \"sum\": 210.6950283050537, \"min\": 210.6950283050537}}, \"EndTime\": 1587786346.228724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786346.017626}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:57 INFO 140327940233024] Epoch[33] Batch[0] avg_epoch_loss=2.691497\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:45:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.69149661064\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:06 INFO 140327940233024] Epoch[33] Batch[5] avg_epoch_loss=2.680417\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.68041725953\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:06 INFO 140327940233024] Epoch[33] Batch [5]#011Speed: 67.76 samples/sec#011loss=2.680417\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:14 INFO 140327940233024] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28261.445999145508, \"sum\": 28261.445999145508, \"min\": 28261.445999145508}}, \"EndTime\": 1587786374.4903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786346.228789}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:14 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.0789067326 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:14 INFO 140327940233024] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.68712565899\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:14 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:25 INFO 140327940233024] Epoch[34] Batch[0] avg_epoch_loss=2.677028\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.67702794075\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:35 INFO 140327940233024] Epoch[34] Batch[5] avg_epoch_loss=2.699577\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:35 INFO 140327940233024] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.69957669576\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:35 INFO 140327940233024] Epoch[34] Batch [5]#011Speed: 67.66 samples/sec#011loss=2.699577\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:42 INFO 140327940233024] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28005.42402267456, \"sum\": 28005.42402267456, \"min\": 28005.42402267456}}, \"EndTime\": 1587786402.496272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786374.490372}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:42 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6342518252 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:42 INFO 140327940233024] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.68571836948\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:42 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:53 INFO 140327940233024] Epoch[35] Batch[0] avg_epoch_loss=2.673414\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:46:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.67341399193\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:02 INFO 140327940233024] Epoch[35] Batch[5] avg_epoch_loss=2.645565\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.64556491375\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:02 INFO 140327940233024] Epoch[35] Batch [5]#011Speed: 69.28 samples/sec#011loss=2.645565\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] Epoch[35] Batch[10] avg_epoch_loss=2.601819\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=2.54932360649\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] Epoch[35] Batch [10]#011Speed: 68.44 samples/sec#011loss=2.549324\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29693.91894340515, \"sum\": 29693.91894340515, \"min\": 29693.91894340515}}, \"EndTime\": 1587786432.190678, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786402.496334}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6788383845 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.601818865\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:12 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:23 INFO 140327940233024] Epoch[36] Batch[0] avg_epoch_loss=2.622758\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.62275791168\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:32 INFO 140327940233024] Epoch[36] Batch[5] avg_epoch_loss=2.694517\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.69451717536\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:32 INFO 140327940233024] Epoch[36] Batch [5]#011Speed: 68.16 samples/sec#011loss=2.694517\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] Epoch[36] Batch[10] avg_epoch_loss=2.660216\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=2.61905431747\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] Epoch[36] Batch [10]#011Speed: 67.32 samples/sec#011loss=2.619054\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30017.767906188965, \"sum\": 30017.767906188965, \"min\": 30017.767906188965}}, \"EndTime\": 1587786462.20891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786432.190744}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2408951466 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.66021587632\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:42 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:53 INFO 140327940233024] Epoch[37] Batch[0] avg_epoch_loss=2.711585\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:47:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.71158480644\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:02 INFO 140327940233024] Epoch[37] Batch[5] avg_epoch_loss=2.694276\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.6942756176\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:02 INFO 140327940233024] Epoch[37] Batch [5]#011Speed: 68.77 samples/sec#011loss=2.694276\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:10 INFO 140327940233024] processed a total of 1216 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27920.679807662964, \"sum\": 27920.679807662964, \"min\": 27920.679807662964}}, \"EndTime\": 1587786490.130127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786462.208986}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:10 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.5517837977 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:10 INFO 140327940233024] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.69389677048\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:10 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:21 INFO 140327940233024] Epoch[38] Batch[0] avg_epoch_loss=2.673615\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.67361474037\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:30 INFO 140327940233024] Epoch[38] Batch[5] avg_epoch_loss=2.676270\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.67626972993\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:30 INFO 140327940233024] Epoch[38] Batch [5]#011Speed: 68.36 samples/sec#011loss=2.676270\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] Epoch[38] Batch[10] avg_epoch_loss=2.596891\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=2.50163640976\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] Epoch[38] Batch [10]#011Speed: 67.83 samples/sec#011loss=2.501636\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30131.269931793213, \"sum\": 30131.269931793213, \"min\": 30131.269931793213}}, \"EndTime\": 1587786520.261884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786490.130195}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.0448371577 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.59689094804\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:40 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:51 INFO 140327940233024] Epoch[39] Batch[0] avg_epoch_loss=2.606200\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:48:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.60619974136\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:00 INFO 140327940233024] Epoch[39] Batch[5] avg_epoch_loss=2.684897\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.68489730358\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:00 INFO 140327940233024] Epoch[39] Batch [5]#011Speed: 69.79 samples/sec#011loss=2.684897\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:08 INFO 140327940233024] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28048.691987991333, \"sum\": 28048.691987991333, \"min\": 28048.691987991333}}, \"EndTime\": 1587786548.311056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786520.261951}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:08 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.563438445 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:08 INFO 140327940233024] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.67092881203\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:08 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:19 INFO 140327940233024] Epoch[40] Batch[0] avg_epoch_loss=2.694192\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:19 INFO 140327940233024] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.69419240952\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:28 INFO 140327940233024] Epoch[40] Batch[5] avg_epoch_loss=2.645198\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.64519826571\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:28 INFO 140327940233024] Epoch[40] Batch [5]#011Speed: 68.28 samples/sec#011loss=2.645198\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:36 INFO 140327940233024] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28130.295991897583, \"sum\": 28130.295991897583, \"min\": 28130.295991897583}}, \"EndTime\": 1587786576.441862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786548.31113}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:36 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2225638626 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:36 INFO 140327940233024] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.64863591194\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:36 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:47 INFO 140327940233024] Epoch[41] Batch[0] avg_epoch_loss=2.673211\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:47 INFO 140327940233024] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.6732108593\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:56 INFO 140327940233024] Epoch[41] Batch[5] avg_epoch_loss=2.672821\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.67282060782\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:49:56 INFO 140327940233024] Epoch[41] Batch [5]#011Speed: 69.81 samples/sec#011loss=2.672821\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:04 INFO 140327940233024] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27840.692043304443, \"sum\": 27840.692043304443, \"min\": 27840.692043304443}}, \"EndTime\": 1587786604.283201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786576.441954}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:04 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.0777260314 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:04 INFO 140327940233024] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.63215582371\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:04 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:15 INFO 140327940233024] Epoch[42] Batch[0] avg_epoch_loss=2.669425\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.66942548752\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:24 INFO 140327940233024] Epoch[42] Batch[5] avg_epoch_loss=2.641562\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.64156202475\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:24 INFO 140327940233024] Epoch[42] Batch [5]#011Speed: 69.74 samples/sec#011loss=2.641562\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] Epoch[42] Batch[10] avg_epoch_loss=2.654408\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=2.66982350349\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] Epoch[42] Batch [10]#011Speed: 69.60 samples/sec#011loss=2.669824\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29535.27283668518, \"sum\": 29535.27283668518, \"min\": 29535.27283668518}}, \"EndTime\": 1587786633.81904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786604.283275}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.4394392995 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.65440815145\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:33 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:44 INFO 140327940233024] Epoch[43] Batch[0] avg_epoch_loss=2.777998\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.77799844742\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:54 INFO 140327940233024] Epoch[43] Batch[5] avg_epoch_loss=2.659251\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.65925061703\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:50:54 INFO 140327940233024] Epoch[43] Batch [5]#011Speed: 68.42 samples/sec#011loss=2.659251\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:01 INFO 140327940233024] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28075.37889480591, \"sum\": 28075.37889480591, \"min\": 28075.37889480591}}, \"EndTime\": 1587786661.894857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786633.819108}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6296926451 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.65812914371\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:01 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:13 INFO 140327940233024] Epoch[44] Batch[0] avg_epoch_loss=2.628111\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.6281106472\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:22 INFO 140327940233024] Epoch[44] Batch[5] avg_epoch_loss=2.603769\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.60376938184\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:22 INFO 140327940233024] Epoch[44] Batch [5]#011Speed: 69.42 samples/sec#011loss=2.603769\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:29 INFO 140327940233024] processed a total of 1211 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27833.6820602417, \"sum\": 27833.6820602417, \"min\": 27833.6820602417}}, \"EndTime\": 1587786689.729073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786661.894922}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:29 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.5082617523 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:29 INFO 140327940233024] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.57503755093\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:29 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:29 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_07869d17-003f-43c7-a9a8-bcdc027874a9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 196.2110996246338, \"sum\": 196.2110996246338, \"min\": 196.2110996246338}}, \"EndTime\": 1587786689.926007, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786689.729148}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:41 INFO 140327940233024] Epoch[45] Batch[0] avg_epoch_loss=2.551016\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.55101585388\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:50 INFO 140327940233024] Epoch[45] Batch[5] avg_epoch_loss=2.654540\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.65454014142\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:51:50 INFO 140327940233024] Epoch[45] Batch [5]#011Speed: 68.39 samples/sec#011loss=2.654540\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] Epoch[45] Batch[10] avg_epoch_loss=2.664741\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=2.67698197365\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] Epoch[45] Batch [10]#011Speed: 67.49 samples/sec#011loss=2.676982\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] processed a total of 1362 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30075.977087020874, \"sum\": 30075.977087020874, \"min\": 30075.977087020874}}, \"EndTime\": 1587786720.002108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786689.92607}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.2851506133 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.66474097425\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:00 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:11 INFO 140327940233024] Epoch[46] Batch[0] avg_epoch_loss=2.599263\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.5992629528\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:20 INFO 140327940233024] Epoch[46] Batch[5] avg_epoch_loss=2.642541\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.64254124959\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:20 INFO 140327940233024] Epoch[46] Batch [5]#011Speed: 69.26 samples/sec#011loss=2.642541\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:27 INFO 140327940233024] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27886.234045028687, \"sum\": 27886.234045028687, \"min\": 27886.234045028687}}, \"EndTime\": 1587786747.888815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786720.002181}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:27 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.6854598291 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:27 INFO 140327940233024] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.64048581123\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:27 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:39 INFO 140327940233024] Epoch[47] Batch[0] avg_epoch_loss=2.678850\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.67885041237\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:48 INFO 140327940233024] Epoch[47] Batch[5] avg_epoch_loss=2.658551\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.65855117639\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:48 INFO 140327940233024] Epoch[47] Batch [5]#011Speed: 68.10 samples/sec#011loss=2.658551\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:55 INFO 140327940233024] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28067.553997039795, \"sum\": 28067.553997039795, \"min\": 28067.553997039795}}, \"EndTime\": 1587786775.956857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786747.888879}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:55 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.3546988279 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:55 INFO 140327940233024] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.65793488026\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:52:55 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:07 INFO 140327940233024] Epoch[48] Batch[0] avg_epoch_loss=2.686454\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:07 INFO 140327940233024] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.68645358086\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:16 INFO 140327940233024] Epoch[48] Batch[5] avg_epoch_loss=2.633258\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.63325758775\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:16 INFO 140327940233024] Epoch[48] Batch [5]#011Speed: 68.94 samples/sec#011loss=2.633258\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:24 INFO 140327940233024] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28248.649835586548, \"sum\": 28248.649835586548, \"min\": 28248.649835586548}}, \"EndTime\": 1587786804.205995, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786775.956922}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:24 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4975362975 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:24 INFO 140327940233024] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.64425923824\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:24 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:35 INFO 140327940233024] Epoch[49] Batch[0] avg_epoch_loss=2.566430\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:35 INFO 140327940233024] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.56643033028\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:44 INFO 140327940233024] Epoch[49] Batch[5] avg_epoch_loss=2.613412\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.61341150602\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:44 INFO 140327940233024] Epoch[49] Batch [5]#011Speed: 68.43 samples/sec#011loss=2.613412\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] Epoch[49] Batch[10] avg_epoch_loss=2.645598\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=2.68422198296\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] Epoch[49] Batch [10]#011Speed: 67.96 samples/sec#011loss=2.684222\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] processed a total of 1358 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30044.9481010437, \"sum\": 30044.9481010437, \"min\": 30044.9481010437}}, \"EndTime\": 1587786834.251501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786804.206061}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1987961567 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.64559808644\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:53:54 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:05 INFO 140327940233024] Epoch[50] Batch[0] avg_epoch_loss=2.630581\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.63058066368\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:14 INFO 140327940233024] Epoch[50] Batch[5] avg_epoch_loss=2.624013\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.6240127484\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:14 INFO 140327940233024] Epoch[50] Batch [5]#011Speed: 69.57 samples/sec#011loss=2.624013\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:22 INFO 140327940233024] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27892.635107040405, \"sum\": 27892.635107040405, \"min\": 27892.635107040405}}, \"EndTime\": 1587786862.144758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786834.251566}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:22 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.2447467099 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:22 INFO 140327940233024] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.60673787594\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:22 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:33 INFO 140327940233024] Epoch[51] Batch[0] avg_epoch_loss=2.717483\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.71748280525\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:42 INFO 140327940233024] Epoch[51] Batch[5] avg_epoch_loss=2.649839\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.64983852704\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:42 INFO 140327940233024] Epoch[51] Batch [5]#011Speed: 67.97 samples/sec#011loss=2.649839\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:50 INFO 140327940233024] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28251.571893692017, \"sum\": 28251.571893692017, \"min\": 28251.571893692017}}, \"EndTime\": 1587786890.396959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786862.144827}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:50 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.2361533874 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:50 INFO 140327940233024] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.63628642559\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:54:50 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:01 INFO 140327940233024] Epoch[52] Batch[0] avg_epoch_loss=2.552740\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.55274009705\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:10 INFO 140327940233024] Epoch[52] Batch[5] avg_epoch_loss=2.600963\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.60096251965\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:10 INFO 140327940233024] Epoch[52] Batch [5]#011Speed: 69.33 samples/sec#011loss=2.600963\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] Epoch[52] Batch[10] avg_epoch_loss=2.545352\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=2.47861835957\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] Epoch[52] Batch [10]#011Speed: 68.97 samples/sec#011loss=2.478618\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29800.588130950928, \"sum\": 29800.588130950928, \"min\": 29800.588130950928}}, \"EndTime\": 1587786920.198412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786890.397084}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.1198111903 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.54535153779\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:20 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_5e27b182-7d5e-4564-8c9b-248220e6a899-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 198.57311248779297, \"sum\": 198.57311248779297, \"min\": 198.57311248779297}}, \"EndTime\": 1587786920.397498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786920.198483}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:31 INFO 140327940233024] Epoch[53] Batch[0] avg_epoch_loss=2.635850\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.63584971428\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:40 INFO 140327940233024] Epoch[53] Batch[5] avg_epoch_loss=2.612511\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.61251072089\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:40 INFO 140327940233024] Epoch[53] Batch [5]#011Speed: 69.58 samples/sec#011loss=2.612511\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:48 INFO 140327940233024] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27840.038061141968, \"sum\": 27840.038061141968, \"min\": 27840.038061141968}}, \"EndTime\": 1587786948.237652, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786920.397554}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:48 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8991881822 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:48 INFO 140327940233024] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.65120680332\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:48 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:59 INFO 140327940233024] Epoch[54] Batch[0] avg_epoch_loss=2.574623\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:55:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.57462334633\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:08 INFO 140327940233024] Epoch[54] Batch[5] avg_epoch_loss=2.574452\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.57445247968\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:08 INFO 140327940233024] Epoch[54] Batch [5]#011Speed: 68.81 samples/sec#011loss=2.574452\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:16 INFO 140327940233024] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27949.034214019775, \"sum\": 27949.034214019775, \"min\": 27949.034214019775}}, \"EndTime\": 1587786976.18718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786948.237726}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:16 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.5828125552 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:16 INFO 140327940233024] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.59360897541\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:16 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:27 INFO 140327940233024] Epoch[55] Batch[0] avg_epoch_loss=2.696409\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.69640946388\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:36 INFO 140327940233024] Epoch[55] Batch[5] avg_epoch_loss=2.628487\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.62848655383\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:36 INFO 140327940233024] Epoch[55] Batch [5]#011Speed: 69.87 samples/sec#011loss=2.628487\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] Epoch[55] Batch[10] avg_epoch_loss=2.535641\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=2.42422585487\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] Epoch[55] Batch [10]#011Speed: 69.49 samples/sec#011loss=2.424226\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29468.920946121216, \"sum\": 29468.920946121216, \"min\": 29468.920946121216}}, \"EndTime\": 1587787005.656594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587786976.187245}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.5033117255 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.53564078158\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:45 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_7358dd18-416b-4bb4-8733-e8371444f5a8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 195.1889991760254, \"sum\": 195.1889991760254, \"min\": 195.1889991760254}}, \"EndTime\": 1587787005.852308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787005.656658}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:57 INFO 140327940233024] Epoch[56] Batch[0] avg_epoch_loss=2.672772\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:56:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.67277240753\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:06 INFO 140327940233024] Epoch[56] Batch[5] avg_epoch_loss=2.611809\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.61180909475\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:06 INFO 140327940233024] Epoch[56] Batch [5]#011Speed: 67.56 samples/sec#011loss=2.611809\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:13 INFO 140327940233024] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28108.606100082397, \"sum\": 28108.606100082397, \"min\": 28108.606100082397}}, \"EndTime\": 1587787033.961029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787005.852368}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:13 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8971249601 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:13 INFO 140327940233024] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.61841554642\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:13 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:25 INFO 140327940233024] Epoch[57] Batch[0] avg_epoch_loss=2.620613\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.62061262131\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:34 INFO 140327940233024] Epoch[57] Batch[5] avg_epoch_loss=2.587228\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.58722837766\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:34 INFO 140327940233024] Epoch[57] Batch [5]#011Speed: 67.92 samples/sec#011loss=2.587228\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:42 INFO 140327940233024] processed a total of 1214 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28060.06383895874, \"sum\": 28060.06383895874, \"min\": 28060.06383895874}}, \"EndTime\": 1587787062.021726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787033.961093}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:42 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2641713433 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:42 INFO 140327940233024] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.59287190437\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:42 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:53 INFO 140327940233024] Epoch[58] Batch[0] avg_epoch_loss=2.534053\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:57:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.53405332565\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:02 INFO 140327940233024] Epoch[58] Batch[5] avg_epoch_loss=2.620380\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.62037980556\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:02 INFO 140327940233024] Epoch[58] Batch [5]#011Speed: 69.27 samples/sec#011loss=2.620380\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] Epoch[58] Batch[10] avg_epoch_loss=2.581957\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=2.53585057259\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] Epoch[58] Batch [10]#011Speed: 69.16 samples/sec#011loss=2.535851\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29785.04514694214, \"sum\": 29785.04514694214, \"min\": 29785.04514694214}}, \"EndTime\": 1587787091.807299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787062.021794}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.384545907 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.58195742694\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:11 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:22 INFO 140327940233024] Epoch[59] Batch[0] avg_epoch_loss=2.625991\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.6259906292\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:31 INFO 140327940233024] Epoch[59] Batch[5] avg_epoch_loss=2.619974\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.6199742953\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:31 INFO 140327940233024] Epoch[59] Batch [5]#011Speed: 70.07 samples/sec#011loss=2.619974\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:39 INFO 140327940233024] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27569.427013397217, \"sum\": 27569.427013397217, \"min\": 27569.427013397217}}, \"EndTime\": 1587787119.377178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787091.807361}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=46.3917233485 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.59794006348\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:39 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:50 INFO 140327940233024] Epoch[60] Batch[0] avg_epoch_loss=2.584512\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.58451199532\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:59 INFO 140327940233024] Epoch[60] Batch[5] avg_epoch_loss=2.590868\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.59086807569\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:58:59 INFO 140327940233024] Epoch[60] Batch [5]#011Speed: 68.63 samples/sec#011loss=2.590868\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:07 INFO 140327940233024] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28089.087963104248, \"sum\": 28089.087963104248, \"min\": 28089.087963104248}}, \"EndTime\": 1587787147.466887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787119.377254}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:07 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.213121368 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:07 INFO 140327940233024] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:07 INFO 140327940233024] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.6038269043\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:07 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:19 INFO 140327940233024] Epoch[61] Batch[0] avg_epoch_loss=2.643816\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:19 INFO 140327940233024] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.64381623268\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:28 INFO 140327940233024] Epoch[61] Batch[5] avg_epoch_loss=2.607142\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.6071416537\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:28 INFO 140327940233024] Epoch[61] Batch [5]#011Speed: 69.64 samples/sec#011loss=2.607142\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:35 INFO 140327940233024] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28186.262130737305, \"sum\": 28186.262130737305, \"min\": 28186.262130737305}}, \"EndTime\": 1587787175.653645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787147.466954}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:35 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.163689292 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:35 INFO 140327940233024] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:35 INFO 140327940233024] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.60744991302\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:35 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:46 INFO 140327940233024] Epoch[62] Batch[0] avg_epoch_loss=2.617162\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.61716151237\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:56 INFO 140327940233024] Epoch[62] Batch[5] avg_epoch_loss=2.586328\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.58632814884\u001b[0m\n",
      "\u001b[34m[04/25/2020 03:59:56 INFO 140327940233024] Epoch[62] Batch [5]#011Speed: 69.48 samples/sec#011loss=2.586328\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] Epoch[62] Batch[10] avg_epoch_loss=2.586029\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=2.58567070961\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] Epoch[62] Batch [10]#011Speed: 68.98 samples/sec#011loss=2.585671\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29779.611110687256, \"sum\": 29779.611110687256, \"min\": 29779.611110687256}}, \"EndTime\": 1587787205.433767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787175.653709}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.150189555 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.58602931283\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:05 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:16 INFO 140327940233024] Epoch[63] Batch[0] avg_epoch_loss=2.556060\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.55605983734\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:26 INFO 140327940233024] Epoch[63] Batch[5] avg_epoch_loss=2.536428\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:26 INFO 140327940233024] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.53642829259\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:26 INFO 140327940233024] Epoch[63] Batch [5]#011Speed: 67.73 samples/sec#011loss=2.536428\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:33 INFO 140327940233024] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28289.72816467285, \"sum\": 28289.72816467285, \"min\": 28289.72816467285}}, \"EndTime\": 1587787233.723973, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787205.433833}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:33 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1398966124 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:33 INFO 140327940233024] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.54896123409\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:33 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:44 INFO 140327940233024] Epoch[64] Batch[0] avg_epoch_loss=2.562367\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.56236696243\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:54 INFO 140327940233024] Epoch[64] Batch[5] avg_epoch_loss=2.573237\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.57323690255\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:00:54 INFO 140327940233024] Epoch[64] Batch [5]#011Speed: 69.64 samples/sec#011loss=2.573237\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:01 INFO 140327940233024] processed a total of 1198 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27863.067150115967, \"sum\": 27863.067150115967, \"min\": 27863.067150115967}}, \"EndTime\": 1587787261.587553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787233.724039}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.9958118332 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.52234470844\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:01 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:01 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_2e3f5b4a-7ccc-4f08-8571-cd7198ae174d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 216.28093719482422, \"sum\": 216.28093719482422, \"min\": 216.28093719482422}}, \"EndTime\": 1587787261.804672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787261.587626}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:13 INFO 140327940233024] Epoch[65] Batch[0] avg_epoch_loss=2.623816\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.6238155365\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:22 INFO 140327940233024] Epoch[65] Batch[5] avg_epoch_loss=2.570223\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.57022325198\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:22 INFO 140327940233024] Epoch[65] Batch [5]#011Speed: 67.73 samples/sec#011loss=2.570223\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] Epoch[65] Batch[10] avg_epoch_loss=2.511734\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=2.44154782295\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] Epoch[65] Batch [10]#011Speed: 67.68 samples/sec#011loss=2.441548\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30127.120971679688, \"sum\": 30127.120971679688, \"min\": 30127.120971679688}}, \"EndTime\": 1587787291.931927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787261.804745}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.5196869491 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.5117344206\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:31 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:32 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_6ae037b6-5dad-46be-9b0d-31a16c94c4fa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 201.58696174621582, \"sum\": 201.58696174621582, \"min\": 201.58696174621582}}, \"EndTime\": 1587787292.133966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787291.931995}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:43 INFO 140327940233024] Epoch[66] Batch[0] avg_epoch_loss=2.494524\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.49452352524\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:52 INFO 140327940233024] Epoch[66] Batch[5] avg_epoch_loss=2.538017\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.53801723321\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:01:52 INFO 140327940233024] Epoch[66] Batch [5]#011Speed: 68.25 samples/sec#011loss=2.538017\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] Epoch[66] Batch[10] avg_epoch_loss=2.647927\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=2.77981948853\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] Epoch[66] Batch [10]#011Speed: 67.56 samples/sec#011loss=2.779819\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30135.29896736145, \"sum\": 30135.29896736145, \"min\": 30135.29896736145}}, \"EndTime\": 1587787322.269393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787292.134031}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.7072407782 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.64792734926\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:02 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:13 INFO 140327940233024] Epoch[67] Batch[0] avg_epoch_loss=2.513318\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.51331830025\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:22 INFO 140327940233024] Epoch[67] Batch[5] avg_epoch_loss=2.593248\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.59324820836\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:22 INFO 140327940233024] Epoch[67] Batch [5]#011Speed: 67.97 samples/sec#011loss=2.593248\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:30 INFO 140327940233024] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28305.370092391968, \"sum\": 28305.370092391968, \"min\": 28305.370092391968}}, \"EndTime\": 1587787350.575267, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787322.269464}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:30 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.7724518629 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:30 INFO 140327940233024] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.62902061939\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:30 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:41 INFO 140327940233024] Epoch[68] Batch[0] avg_epoch_loss=2.540518\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.54051804543\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:51 INFO 140327940233024] Epoch[68] Batch[5] avg_epoch_loss=2.578997\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.57899701595\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:51 INFO 140327940233024] Epoch[68] Batch [5]#011Speed: 69.55 samples/sec#011loss=2.578997\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:58 INFO 140327940233024] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27870.052099227905, \"sum\": 27870.052099227905, \"min\": 27870.052099227905}}, \"EndTime\": 1587787378.445915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787350.575335}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:58 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.5325795607 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:58 INFO 140327940233024] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:58 INFO 140327940233024] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.57353394032\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:02:58 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:09 INFO 140327940233024] Epoch[69] Batch[0] avg_epoch_loss=2.552156\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.55215573311\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:18 INFO 140327940233024] Epoch[69] Batch[5] avg_epoch_loss=2.542321\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.54232100646\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:18 INFO 140327940233024] Epoch[69] Batch [5]#011Speed: 69.02 samples/sec#011loss=2.542321\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] Epoch[69] Batch[10] avg_epoch_loss=2.551070\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=2.56156888008\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] Epoch[69] Batch [10]#011Speed: 68.03 samples/sec#011loss=2.561569\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29768.606901168823, \"sum\": 29768.606901168823, \"min\": 29768.606901168823}}, \"EndTime\": 1587787408.214966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787378.445983}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.8715654906 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.55107003992\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:28 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:39 INFO 140327940233024] Epoch[70] Batch[0] avg_epoch_loss=2.465832\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.46583175659\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:48 INFO 140327940233024] Epoch[70] Batch[5] avg_epoch_loss=2.551301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.55130084356\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:48 INFO 140327940233024] Epoch[70] Batch [5]#011Speed: 69.75 samples/sec#011loss=2.551301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] Epoch[70] Batch[10] avg_epoch_loss=2.558913\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=2.56804742813\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] Epoch[70] Batch [10]#011Speed: 70.00 samples/sec#011loss=2.568047\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29396.994829177856, \"sum\": 29396.994829177856, \"min\": 29396.994829177856}}, \"EndTime\": 1587787437.612421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787408.215036}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.3446136463 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.55891292745\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:03:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:09 INFO 140327940233024] Epoch[71] Batch[0] avg_epoch_loss=2.544496\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.544495821\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:18 INFO 140327940233024] Epoch[71] Batch[5] avg_epoch_loss=2.571646\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.571646293\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:18 INFO 140327940233024] Epoch[71] Batch [5]#011Speed: 68.36 samples/sec#011loss=2.571646\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:25 INFO 140327940233024] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28306.030988693237, \"sum\": 28306.030988693237, \"min\": 28306.030988693237}}, \"EndTime\": 1587787465.918895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787437.61249}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:25 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.7713756605 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:25 INFO 140327940233024] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.55633299351\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:25 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:37 INFO 140327940233024] Epoch[72] Batch[0] avg_epoch_loss=2.510279\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:37 INFO 140327940233024] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.51027941704\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:46 INFO 140327940233024] Epoch[72] Batch[5] avg_epoch_loss=2.542461\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.54246135553\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:46 INFO 140327940233024] Epoch[72] Batch [5]#011Speed: 68.88 samples/sec#011loss=2.542461\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] Epoch[72] Batch[10] avg_epoch_loss=2.517351\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=2.48721895218\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] Epoch[72] Batch [10]#011Speed: 69.39 samples/sec#011loss=2.487219\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29787.209033966064, \"sum\": 29787.209033966064, \"min\": 29787.209033966064}}, \"EndTime\": 1587787495.706668, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787465.919002}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8177459004 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.51735117219\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:04:55 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:06 INFO 140327940233024] Epoch[73] Batch[0] avg_epoch_loss=2.597993\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.59799289703\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:16 INFO 140327940233024] Epoch[73] Batch[5] avg_epoch_loss=2.560620\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.56062034766\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:16 INFO 140327940233024] Epoch[73] Batch [5]#011Speed: 69.34 samples/sec#011loss=2.560620\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:23 INFO 140327940233024] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27854.36201095581, \"sum\": 27854.36201095581, \"min\": 27854.36201095581}}, \"EndTime\": 1587787523.561498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787495.706731}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:23 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5529717538 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:23 INFO 140327940233024] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.51961474419\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:23 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:34 INFO 140327940233024] Epoch[74] Batch[0] avg_epoch_loss=2.612570\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.6125702858\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:44 INFO 140327940233024] Epoch[74] Batch[5] avg_epoch_loss=2.539017\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.53901692231\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:44 INFO 140327940233024] Epoch[74] Batch [5]#011Speed: 67.56 samples/sec#011loss=2.539017\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] Epoch[74] Batch[10] avg_epoch_loss=2.530645\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=2.52059774399\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] Epoch[74] Batch [10]#011Speed: 68.19 samples/sec#011loss=2.520598\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30105.27801513672, \"sum\": 30105.27801513672, \"min\": 30105.27801513672}}, \"EndTime\": 1587787553.667234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787523.561582}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.5802517364 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.53064456853\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:05:53 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:05 INFO 140327940233024] Epoch[75] Batch[0] avg_epoch_loss=2.617505\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.61750459671\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:14 INFO 140327940233024] Epoch[75] Batch[5] avg_epoch_loss=2.599965\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.5999648571\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:14 INFO 140327940233024] Epoch[75] Batch [5]#011Speed: 68.84 samples/sec#011loss=2.599965\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] Epoch[75] Batch[10] avg_epoch_loss=2.580615\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=2.55739526749\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] Epoch[75] Batch [10]#011Speed: 69.41 samples/sec#011loss=2.557395\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29892.443895339966, \"sum\": 29892.443895339966, \"min\": 29892.443895339966}}, \"EndTime\": 1587787583.560127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787553.667303}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.4221991493 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.58061504364\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:23 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:34 INFO 140327940233024] Epoch[76] Batch[0] avg_epoch_loss=2.598474\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.59847402573\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:44 INFO 140327940233024] Epoch[76] Batch[5] avg_epoch_loss=2.559094\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.55909391244\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:44 INFO 140327940233024] Epoch[76] Batch [5]#011Speed: 67.89 samples/sec#011loss=2.559094\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:51 INFO 140327940233024] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28203.775882720947, \"sum\": 28203.775882720947, \"min\": 28203.775882720947}}, \"EndTime\": 1587787611.764382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787583.560193}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:51 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.3201479439 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:51 INFO 140327940233024] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.56041867733\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:06:51 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:03 INFO 140327940233024] Epoch[77] Batch[0] avg_epoch_loss=2.549999\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:03 INFO 140327940233024] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.54999923706\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:12 INFO 140327940233024] Epoch[77] Batch[5] avg_epoch_loss=2.570435\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.57043484847\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:12 INFO 140327940233024] Epoch[77] Batch [5]#011Speed: 69.28 samples/sec#011loss=2.570435\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] Epoch[77] Batch[10] avg_epoch_loss=2.549263\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=2.52385601997\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] Epoch[77] Batch [10]#011Speed: 68.97 samples/sec#011loss=2.523856\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29787.137985229492, \"sum\": 29787.137985229492, \"min\": 29787.137985229492}}, \"EndTime\": 1587787641.552018, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787611.764449}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6164288906 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.5492626537\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:21 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:32 INFO 140327940233024] Epoch[78] Batch[0] avg_epoch_loss=2.586381\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.58638143539\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:42 INFO 140327940233024] Epoch[78] Batch[5] avg_epoch_loss=2.563512\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.56351248423\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:42 INFO 140327940233024] Epoch[78] Batch [5]#011Speed: 68.12 samples/sec#011loss=2.563512\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:49 INFO 140327940233024] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28157.972812652588, \"sum\": 28157.972812652588, \"min\": 28157.972812652588}}, \"EndTime\": 1587787669.710434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787641.55208}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:49 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.3866174908 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:49 INFO 140327940233024] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.56059439182\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:07:49 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:00 INFO 140327940233024] Epoch[79] Batch[0] avg_epoch_loss=2.521957\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.52195739746\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:10 INFO 140327940233024] Epoch[79] Batch[5] avg_epoch_loss=2.562755\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.56275455157\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:10 INFO 140327940233024] Epoch[79] Batch [5]#011Speed: 69.26 samples/sec#011loss=2.562755\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] Epoch[79] Batch[10] avg_epoch_loss=2.509827\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=2.4463136673\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] Epoch[79] Batch [10]#011Speed: 69.03 samples/sec#011loss=2.446314\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29787.344932556152, \"sum\": 29787.344932556152, \"min\": 29787.344932556152}}, \"EndTime\": 1587787699.498491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787669.710506}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.3739871132 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.5098268769\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:19 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_667dbb50-06be-4a5a-a1b1-fc16d3964d0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 197.89409637451172, \"sum\": 197.89409637451172, \"min\": 197.89409637451172}}, \"EndTime\": 1587787699.696802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787699.498555}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:30 INFO 140327940233024] Epoch[80] Batch[0] avg_epoch_loss=2.506859\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.50685858727\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:40 INFO 140327940233024] Epoch[80] Batch[5] avg_epoch_loss=2.573395\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.57339537144\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:40 INFO 140327940233024] Epoch[80] Batch [5]#011Speed: 68.57 samples/sec#011loss=2.573395\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] Epoch[80] Batch[10] avg_epoch_loss=2.617504\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=2.67043347359\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] Epoch[80] Batch [10]#011Speed: 68.25 samples/sec#011loss=2.670433\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29758.8369846344, \"sum\": 29758.8369846344, \"min\": 29758.8369846344}}, \"EndTime\": 1587787729.45575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787699.696858}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6171156393 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.61750359969\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:08:49 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:00 INFO 140327940233024] Epoch[81] Batch[0] avg_epoch_loss=2.629373\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.62937283516\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:09 INFO 140327940233024] Epoch[81] Batch[5] avg_epoch_loss=2.587041\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.58704078197\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:09 INFO 140327940233024] Epoch[81] Batch [5]#011Speed: 70.17 samples/sec#011loss=2.587041\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] Epoch[81] Batch[10] avg_epoch_loss=2.520404\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=2.44043955803\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] Epoch[81] Batch [10]#011Speed: 69.41 samples/sec#011loss=2.440440\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29365.461826324463, \"sum\": 29365.461826324463, \"min\": 29365.461826324463}}, \"EndTime\": 1587787758.821687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787729.455839}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6225295213 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.520403862\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:18 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:29 INFO 140327940233024] Epoch[82] Batch[0] avg_epoch_loss=2.566139\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.56613874435\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:39 INFO 140327940233024] Epoch[82] Batch[5] avg_epoch_loss=2.586987\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.58698741595\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:39 INFO 140327940233024] Epoch[82] Batch [5]#011Speed: 68.24 samples/sec#011loss=2.586987\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] Epoch[82] Batch[10] avg_epoch_loss=2.531540\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=2.46500372887\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] Epoch[82] Batch [10]#011Speed: 68.10 samples/sec#011loss=2.465004\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] processed a total of 1375 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29839.202165603638, \"sum\": 29839.202165603638, \"min\": 29839.202165603638}}, \"EndTime\": 1587787788.661296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787758.82175}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=46.0801728825 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.53154028546\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:48 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:59 INFO 140327940233024] Epoch[83] Batch[0] avg_epoch_loss=2.592985\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:09:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.59298467636\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:09 INFO 140327940233024] Epoch[83] Batch[5] avg_epoch_loss=2.551652\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.55165159702\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:09 INFO 140327940233024] Epoch[83] Batch [5]#011Speed: 69.43 samples/sec#011loss=2.551652\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] Epoch[83] Batch[10] avg_epoch_loss=2.661343\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=2.79297194481\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] Epoch[83] Batch [10]#011Speed: 69.14 samples/sec#011loss=2.792972\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29651.516914367676, \"sum\": 29651.516914367676, \"min\": 29651.516914367676}}, \"EndTime\": 1587787818.313259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787788.661359}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.5052245929 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.6613426642\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:18 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:29 INFO 140327940233024] Epoch[84] Batch[0] avg_epoch_loss=2.591821\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.59182143211\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:39 INFO 140327940233024] Epoch[84] Batch[5] avg_epoch_loss=2.531191\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.53119067351\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:39 INFO 140327940233024] Epoch[84] Batch [5]#011Speed: 68.45 samples/sec#011loss=2.531191\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] Epoch[84] Batch[10] avg_epoch_loss=2.490193\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=2.44099607468\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] Epoch[84] Batch [10]#011Speed: 67.80 samples/sec#011loss=2.440996\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30166.63384437561, \"sum\": 30166.63384437561, \"min\": 30166.63384437561}}, \"EndTime\": 1587787848.480346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787818.313323}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.8286394688 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.49019312859\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:48 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_17887cb8-f713-483e-9eda-f5588bbbe1d6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 196.2428092956543, \"sum\": 196.2428092956543, \"min\": 196.2428092956543}}, \"EndTime\": 1587787848.677049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787848.48041}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:59 INFO 140327940233024] Epoch[85] Batch[0] avg_epoch_loss=2.615937\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:10:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=2.61593651772\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:08 INFO 140327940233024] Epoch[85] Batch[5] avg_epoch_loss=2.524761\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.52476119995\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:08 INFO 140327940233024] Epoch[85] Batch [5]#011Speed: 69.00 samples/sec#011loss=2.524761\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:16 INFO 140327940233024] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27729.209184646606, \"sum\": 27729.209184646606, \"min\": 27729.209184646606}}, \"EndTime\": 1587787876.406384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787848.677117}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:16 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.465577246 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:16 INFO 140327940233024] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.51293582916\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:16 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:27 INFO 140327940233024] Epoch[86] Batch[0] avg_epoch_loss=2.580301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=2.58030080795\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:36 INFO 140327940233024] Epoch[86] Batch[5] avg_epoch_loss=2.533131\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.53313108285\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:36 INFO 140327940233024] Epoch[86] Batch [5]#011Speed: 69.79 samples/sec#011loss=2.533131\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:44 INFO 140327940233024] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27737.329959869385, \"sum\": 27737.329959869385, \"min\": 27737.329959869385}}, \"EndTime\": 1587787904.144288, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787876.406456}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:44 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1735894087 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:44 INFO 140327940233024] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.51482572556\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:44 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:55 INFO 140327940233024] Epoch[87] Batch[0] avg_epoch_loss=2.585046\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:11:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.58504581451\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:04 INFO 140327940233024] Epoch[87] Batch[5] avg_epoch_loss=2.531759\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.53175894419\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:04 INFO 140327940233024] Epoch[87] Batch [5]#011Speed: 69.69 samples/sec#011loss=2.531759\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] Epoch[87] Batch[10] avg_epoch_loss=2.505957\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=2.47499442101\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] Epoch[87] Batch [10]#011Speed: 69.73 samples/sec#011loss=2.474994\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29499.228954315186, \"sum\": 29499.228954315186, \"min\": 29499.228954315186}}, \"EndTime\": 1587787933.644083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787904.144365}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.831507093 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.5059568882\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:13 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:24 INFO 140327940233024] Epoch[88] Batch[0] avg_epoch_loss=2.569797\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.56979727745\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:34 INFO 140327940233024] Epoch[88] Batch[5] avg_epoch_loss=2.570074\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.57007431984\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:34 INFO 140327940233024] Epoch[88] Batch [5]#011Speed: 69.65 samples/sec#011loss=2.570074\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] Epoch[88] Batch[10] avg_epoch_loss=2.582819\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=2.59811177254\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] Epoch[88] Batch [10]#011Speed: 69.43 samples/sec#011loss=2.598112\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29593.163013458252, \"sum\": 29593.163013458252, \"min\": 29593.163013458252}}, \"EndTime\": 1587787963.237776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787933.644152}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4695640914 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=88, train loss <loss>=2.58281861652\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:43 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:54 INFO 140327940233024] Epoch[89] Batch[0] avg_epoch_loss=2.579202\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:12:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=2.57920217514\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:03 INFO 140327940233024] Epoch[89] Batch[5] avg_epoch_loss=2.575292\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:03 INFO 140327940233024] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=2.5752915144\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:03 INFO 140327940233024] Epoch[89] Batch [5]#011Speed: 69.39 samples/sec#011loss=2.575292\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:11 INFO 140327940233024] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27767.239093780518, \"sum\": 27767.239093780518, \"min\": 27767.239093780518}}, \"EndTime\": 1587787991.005469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787963.237842}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:11 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5847643571 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:11 INFO 140327940233024] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=89, train loss <loss>=2.56292879581\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:11 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:22 INFO 140327940233024] Epoch[90] Batch[0] avg_epoch_loss=2.519472\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=2.51947164536\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:31 INFO 140327940233024] Epoch[90] Batch[5] avg_epoch_loss=2.538678\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=2.53867785136\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:31 INFO 140327940233024] Epoch[90] Batch [5]#011Speed: 68.39 samples/sec#011loss=2.538678\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:39 INFO 140327940233024] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28097.942113876343, \"sum\": 28097.942113876343, \"min\": 28097.942113876343}}, \"EndTime\": 1587788019.103887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587787991.005535}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.4835882074 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=90, train loss <loss>=2.55116894245\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:39 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:50 INFO 140327940233024] Epoch[91] Batch[0] avg_epoch_loss=2.558885\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=2.55888485909\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:59 INFO 140327940233024] Epoch[91] Batch[5] avg_epoch_loss=2.546153\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.54615279039\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:13:59 INFO 140327940233024] Epoch[91] Batch [5]#011Speed: 70.12 samples/sec#011loss=2.546153\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] Epoch[91] Batch[10] avg_epoch_loss=2.522893\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=2.49498095512\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] Epoch[91] Batch [10]#011Speed: 69.80 samples/sec#011loss=2.494981\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29524.36590194702, \"sum\": 29524.36590194702, \"min\": 29524.36590194702}}, \"EndTime\": 1587788048.628765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788019.103956}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4377290622 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.52289286527\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:08 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:19 INFO 140327940233024] Epoch[92] Batch[0] avg_epoch_loss=2.446145\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:19 INFO 140327940233024] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=2.4461452961\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:29 INFO 140327940233024] Epoch[92] Batch[5] avg_epoch_loss=2.535746\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.53574573994\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:29 INFO 140327940233024] Epoch[92] Batch [5]#011Speed: 68.62 samples/sec#011loss=2.535746\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:36 INFO 140327940233024] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27963.47713470459, \"sum\": 27963.47713470459, \"min\": 27963.47713470459}}, \"EndTime\": 1587788076.592665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788048.628828}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:36 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.3433921611 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:36 INFO 140327940233024] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.53858373165\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:36 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:47 INFO 140327940233024] Epoch[93] Batch[0] avg_epoch_loss=2.578054\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:47 INFO 140327940233024] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=2.5780544281\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:56 INFO 140327940233024] Epoch[93] Batch[5] avg_epoch_loss=2.562472\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=2.56247162819\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:14:56 INFO 140327940233024] Epoch[93] Batch [5]#011Speed: 70.14 samples/sec#011loss=2.562472\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:04 INFO 140327940233024] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27557.32297897339, \"sum\": 27557.32297897339, \"min\": 27557.32297897339}}, \"EndTime\": 1587788104.150434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788076.592735}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:04 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.7791925769 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:04 INFO 140327940233024] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.53688952923\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:04 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:15 INFO 140327940233024] Epoch[94] Batch[0] avg_epoch_loss=2.430906\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.43090581894\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:24 INFO 140327940233024] Epoch[94] Batch[5] avg_epoch_loss=2.551168\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.55116812388\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:24 INFO 140327940233024] Epoch[94] Batch [5]#011Speed: 69.77 samples/sec#011loss=2.551168\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:31 INFO 140327940233024] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27752.347946166992, \"sum\": 27752.347946166992, \"min\": 27752.347946166992}}, \"EndTime\": 1587788131.903369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788104.150515}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:31 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.9058301607 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:31 INFO 140327940233024] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=94, train loss <loss>=2.55081529617\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:31 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:43 INFO 140327940233024] Epoch[95] Batch[0] avg_epoch_loss=2.464468\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=2.46446824074\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:52 INFO 140327940233024] Epoch[95] Batch[5] avg_epoch_loss=2.549711\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=2.54971138636\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:15:52 INFO 140327940233024] Epoch[95] Batch [5]#011Speed: 68.02 samples/sec#011loss=2.549711\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] Epoch[95] Batch[10] avg_epoch_loss=2.518067\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=2.48009295464\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] Epoch[95] Batch [10]#011Speed: 68.25 samples/sec#011loss=2.480093\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30028.429985046387, \"sum\": 30028.429985046387, \"min\": 30028.429985046387}}, \"EndTime\": 1587788161.932433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788131.903447}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2588520627 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=95, train loss <loss>=2.51806664467\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:01 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:13 INFO 140327940233024] Epoch[96] Batch[0] avg_epoch_loss=2.538991\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=2.53899121284\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:22 INFO 140327940233024] Epoch[96] Batch[5] avg_epoch_loss=2.536011\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.53601117929\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:22 INFO 140327940233024] Epoch[96] Batch [5]#011Speed: 69.51 samples/sec#011loss=2.536011\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:29 INFO 140327940233024] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28004.45008277893, \"sum\": 28004.45008277893, \"min\": 28004.45008277893}}, \"EndTime\": 1587788189.937373, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788161.932501}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:29 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1712292127 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:29 INFO 140327940233024] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.53807852268\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:29 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:41 INFO 140327940233024] Epoch[97] Batch[0] avg_epoch_loss=2.587091\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=2.58709096909\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:50 INFO 140327940233024] Epoch[97] Batch[5] avg_epoch_loss=2.567235\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.56723487377\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:50 INFO 140327940233024] Epoch[97] Batch [5]#011Speed: 69.60 samples/sec#011loss=2.567235\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:57 INFO 140327940233024] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27720.134973526, \"sum\": 27720.134973526, \"min\": 27720.134973526}}, \"EndTime\": 1587788217.658021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788189.937442}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.8509922161 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.56428177357\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:16:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:08 INFO 140327940233024] Epoch[98] Batch[0] avg_epoch_loss=2.586829\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=2.58682894707\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:17 INFO 140327940233024] Epoch[98] Batch[5] avg_epoch_loss=2.549267\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=2.54926709334\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:17 INFO 140327940233024] Epoch[98] Batch [5]#011Speed: 69.76 samples/sec#011loss=2.549267\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:25 INFO 140327940233024] processed a total of 1202 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27759.721040725708, \"sum\": 27759.721040725708, \"min\": 27759.721040725708}}, \"EndTime\": 1587788245.418401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788217.658084}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:25 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2999970661 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:25 INFO 140327940233024] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.55393066406\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:25 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:36 INFO 140327940233024] Epoch[99] Batch[0] avg_epoch_loss=2.511834\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=2.51183390617\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:46 INFO 140327940233024] Epoch[99] Batch[5] avg_epoch_loss=2.529566\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=2.52956604958\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:46 INFO 140327940233024] Epoch[99] Batch [5]#011Speed: 67.66 samples/sec#011loss=2.529566\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] Epoch[99] Batch[10] avg_epoch_loss=2.556851\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=2.58959364891\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] Epoch[99] Batch [10]#011Speed: 67.90 samples/sec#011loss=2.589594\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30010.512828826904, \"sum\": 30010.512828826904, \"min\": 30010.512828826904}}, \"EndTime\": 1587788275.429391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788245.418467}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.817833489 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=99, train loss <loss>=2.556851322\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:17:55 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:06 INFO 140327940233024] Epoch[100] Batch[0] avg_epoch_loss=2.564382\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.56438231468\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:15 INFO 140327940233024] Epoch[100] Batch[5] avg_epoch_loss=2.533284\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.53328402837\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:15 INFO 140327940233024] Epoch[100] Batch [5]#011Speed: 69.57 samples/sec#011loss=2.533284\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:23 INFO 140327940233024] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27822.30019569397, \"sum\": 27822.30019569397, \"min\": 27822.30019569397}}, \"EndTime\": 1587788303.252111, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788275.429459}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:23 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.9637477013 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:23 INFO 140327940233024] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.51325528622\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:23 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:34 INFO 140327940233024] Epoch[101] Batch[0] avg_epoch_loss=2.511996\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=2.51199555397\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:43 INFO 140327940233024] Epoch[101] Batch[5] avg_epoch_loss=2.533569\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=2.53356873989\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:43 INFO 140327940233024] Epoch[101] Batch [5]#011Speed: 68.78 samples/sec#011loss=2.533569\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:51 INFO 140327940233024] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27894.026041030884, \"sum\": 27894.026041030884, \"min\": 27894.026041030884}}, \"EndTime\": 1587788331.146639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788303.252188}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:51 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.0632506654 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:51 INFO 140327940233024] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=101, train loss <loss>=2.51250700951\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:18:51 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:02 INFO 140327940233024] Epoch[102] Batch[0] avg_epoch_loss=2.533818\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=2.53381824493\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:11 INFO 140327940233024] Epoch[102] Batch[5] avg_epoch_loss=2.511265\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=2.51126499971\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:11 INFO 140327940233024] Epoch[102] Batch [5]#011Speed: 70.14 samples/sec#011loss=2.511265\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] Epoch[102] Batch[10] avg_epoch_loss=2.510599\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=2.50980052948\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] Epoch[102] Batch [10]#011Speed: 69.28 samples/sec#011loss=2.509801\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29603.79195213318, \"sum\": 29603.79195213318, \"min\": 29603.79195213318}}, \"EndTime\": 1587788360.750827, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788331.146707}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8589587969 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=102, train loss <loss>=2.51059933142\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:20 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:31 INFO 140327940233024] Epoch[103] Batch[0] avg_epoch_loss=2.582408\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=2.58240771294\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:41 INFO 140327940233024] Epoch[103] Batch[5] avg_epoch_loss=2.523416\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=2.52341576417\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:41 INFO 140327940233024] Epoch[103] Batch [5]#011Speed: 68.87 samples/sec#011loss=2.523416\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] Epoch[103] Batch[10] avg_epoch_loss=2.452757\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=2.36796646118\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] Epoch[103] Batch [10]#011Speed: 67.55 samples/sec#011loss=2.367966\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29982.129096984863, \"sum\": 29982.129096984863, \"min\": 29982.129096984863}}, \"EndTime\": 1587788390.733424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788360.750898}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.3590219158 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=103, train loss <loss>=2.45275699009\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:19:50 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_63a24c0c-9e9e-440e-814c-f6c93bb81e46-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 195.25408744812012, \"sum\": 195.25408744812012, \"min\": 195.25408744812012}}, \"EndTime\": 1587788390.92916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788390.733489}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:02 INFO 140327940233024] Epoch[104] Batch[0] avg_epoch_loss=2.570904\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=2.57090377808\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:11 INFO 140327940233024] Epoch[104] Batch[5] avg_epoch_loss=2.559459\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=2.55945940812\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:11 INFO 140327940233024] Epoch[104] Batch [5]#011Speed: 69.23 samples/sec#011loss=2.559459\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:18 INFO 140327940233024] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28013.722896575928, \"sum\": 28013.722896575928, \"min\": 28013.722896575928}}, \"EndTime\": 1587788418.94301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788390.92922}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:18 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2995429424 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:18 INFO 140327940233024] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=104, train loss <loss>=2.57634224892\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:18 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:30 INFO 140327940233024] Epoch[105] Batch[0] avg_epoch_loss=2.641032\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=2.64103198051\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:39 INFO 140327940233024] Epoch[105] Batch[5] avg_epoch_loss=2.518757\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.51875694593\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:39 INFO 140327940233024] Epoch[105] Batch [5]#011Speed: 68.54 samples/sec#011loss=2.518757\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] Epoch[105] Batch[10] avg_epoch_loss=2.521891\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=2.52565264702\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] Epoch[105] Batch [10]#011Speed: 67.91 samples/sec#011loss=2.525653\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30164.9010181427, \"sum\": 30164.9010181427, \"min\": 30164.9010181427}}, \"EndTime\": 1587788449.108425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788418.943081}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6930245319 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.52189135551\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:20:49 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:00 INFO 140327940233024] Epoch[106] Batch[0] avg_epoch_loss=2.513896\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=2.51389551163\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:09 INFO 140327940233024] Epoch[106] Batch[5] avg_epoch_loss=2.519028\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=2.51902830601\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:09 INFO 140327940233024] Epoch[106] Batch [5]#011Speed: 69.08 samples/sec#011loss=2.519028\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:17 INFO 140327940233024] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28018.47004890442, \"sum\": 28018.47004890442, \"min\": 28018.47004890442}}, \"EndTime\": 1587788477.127273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788449.10849}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:17 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.4341659802 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:17 INFO 140327940233024] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=106, train loss <loss>=2.52805461884\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:17 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:28 INFO 140327940233024] Epoch[107] Batch[0] avg_epoch_loss=2.461894\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=2.46189427376\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:37 INFO 140327940233024] Epoch[107] Batch[5] avg_epoch_loss=2.515175\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:37 INFO 140327940233024] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=2.51517458757\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:37 INFO 140327940233024] Epoch[107] Batch [5]#011Speed: 69.97 samples/sec#011loss=2.515175\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:44 INFO 140327940233024] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27543.330192565918, \"sum\": 27543.330192565918, \"min\": 27543.330192565918}}, \"EndTime\": 1587788504.671103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788477.127335}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:44 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.8548520796 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:44 INFO 140327940233024] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=107, train loss <loss>=2.52766714096\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:44 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:55 INFO 140327940233024] Epoch[108] Batch[0] avg_epoch_loss=2.490063\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:21:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=2.49006319046\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:04 INFO 140327940233024] Epoch[108] Batch[5] avg_epoch_loss=2.513951\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=2.51395130157\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:04 INFO 140327940233024] Epoch[108] Batch [5]#011Speed: 69.56 samples/sec#011loss=2.513951\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:12 INFO 140327940233024] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27685.061931610107, \"sum\": 27685.061931610107, \"min\": 27685.061931610107}}, \"EndTime\": 1587788532.356683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788504.67117}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:12 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.8006392999 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:12 INFO 140327940233024] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=108, train loss <loss>=2.51084730625\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:12 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:23 INFO 140327940233024] Epoch[109] Batch[0] avg_epoch_loss=2.565090\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=2.56509041786\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:32 INFO 140327940233024] Epoch[109] Batch[5] avg_epoch_loss=2.517892\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=2.51789244016\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:32 INFO 140327940233024] Epoch[109] Batch [5]#011Speed: 68.45 samples/sec#011loss=2.517892\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:40 INFO 140327940233024] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27860.00394821167, \"sum\": 27860.00394821167, \"min\": 27860.00394821167}}, \"EndTime\": 1587788560.217189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788532.356788}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:40 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.1849672937 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:40 INFO 140327940233024] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=109, train loss <loss>=2.55458264351\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:40 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:51 INFO 140327940233024] Epoch[110] Batch[0] avg_epoch_loss=2.561165\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:22:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.56116485596\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:00 INFO 140327940233024] Epoch[110] Batch[5] avg_epoch_loss=2.541614\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.54161413511\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:00 INFO 140327940233024] Epoch[110] Batch [5]#011Speed: 69.82 samples/sec#011loss=2.541614\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] Epoch[110] Batch[10] avg_epoch_loss=2.545667\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=2.55053024292\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] Epoch[110] Batch [10]#011Speed: 69.01 samples/sec#011loss=2.550530\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29818.299055099487, \"sum\": 29818.299055099487, \"min\": 29818.299055099487}}, \"EndTime\": 1587788590.035962, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788560.217307}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.1277474857 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.54566691139\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:10 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:21 INFO 140327940233024] Epoch[111] Batch[0] avg_epoch_loss=2.599091\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=2.59909129143\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:30 INFO 140327940233024] Epoch[111] Batch[5] avg_epoch_loss=2.530027\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=2.53002734979\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:30 INFO 140327940233024] Epoch[111] Batch [5]#011Speed: 68.28 samples/sec#011loss=2.530027\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] Epoch[111] Batch[10] avg_epoch_loss=2.538358\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=2.54835391045\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] Epoch[111] Batch [10]#011Speed: 68.23 samples/sec#011loss=2.548354\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29975.332975387573, \"sum\": 29975.332975387573, \"min\": 29975.332975387573}}, \"EndTime\": 1587788620.01187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788590.036024}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.1361267756 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=111, train loss <loss>=2.53835760463\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:40 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:51 INFO 140327940233024] Epoch[112] Batch[0] avg_epoch_loss=2.498094\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:23:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=2.49809360504\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:00 INFO 140327940233024] Epoch[112] Batch[5] avg_epoch_loss=2.508390\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=2.5083895127\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:00 INFO 140327940233024] Epoch[112] Batch [5]#011Speed: 70.15 samples/sec#011loss=2.508390\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] Epoch[112] Batch[10] avg_epoch_loss=2.532179\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=2.56072702408\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] Epoch[112] Batch [10]#011Speed: 69.72 samples/sec#011loss=2.560727\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29559.42702293396, \"sum\": 29559.42702293396, \"min\": 29559.42702293396}}, \"EndTime\": 1587788649.57177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788620.011935}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.3511726798 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=112, train loss <loss>=2.5321792906\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:09 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:20 INFO 140327940233024] Epoch[113] Batch[0] avg_epoch_loss=2.522396\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=2.52239632607\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:30 INFO 140327940233024] Epoch[113] Batch[5] avg_epoch_loss=2.521130\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=2.5211302042\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:30 INFO 140327940233024] Epoch[113] Batch [5]#011Speed: 68.89 samples/sec#011loss=2.521130\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] Epoch[113] Batch[10] avg_epoch_loss=2.501013\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=2.47687149048\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] Epoch[113] Batch [10]#011Speed: 68.66 samples/sec#011loss=2.476871\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29789.680004119873, \"sum\": 29789.680004119873, \"min\": 29789.680004119873}}, \"EndTime\": 1587788679.361921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788649.571846}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.3776069295 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=113, train loss <loss>=2.50101260705\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:39 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:50 INFO 140327940233024] Epoch[114] Batch[0] avg_epoch_loss=2.535933\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:24:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=2.53593277931\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:00 INFO 140327940233024] Epoch[114] Batch[5] avg_epoch_loss=2.490773\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=2.49077308178\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:00 INFO 140327940233024] Epoch[114] Batch [5]#011Speed: 68.17 samples/sec#011loss=2.490773\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:07 INFO 140327940233024] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28294.589042663574, \"sum\": 28294.589042663574, \"min\": 28294.589042663574}}, \"EndTime\": 1587788707.656978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788679.362003}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:07 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.0966908188 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:07 INFO 140327940233024] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:07 INFO 140327940233024] #quality_metric: host=algo-1, epoch=114, train loss <loss>=2.50324418545\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:07 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:18 INFO 140327940233024] Epoch[115] Batch[0] avg_epoch_loss=2.449126\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=2.44912600517\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:28 INFO 140327940233024] Epoch[115] Batch[5] avg_epoch_loss=2.523691\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=2.52369137605\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:28 INFO 140327940233024] Epoch[115] Batch [5]#011Speed: 68.46 samples/sec#011loss=2.523691\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] Epoch[115] Batch[10] avg_epoch_loss=2.555485\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=2.59363713264\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] Epoch[115] Batch [10]#011Speed: 68.50 samples/sec#011loss=2.593637\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30030.659914016724, \"sum\": 30030.659914016724, \"min\": 30030.659914016724}}, \"EndTime\": 1587788737.688383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788707.657109}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.7894472335 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] #quality_metric: host=algo-1, epoch=115, train loss <loss>=2.55548490178\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:37 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:48 INFO 140327940233024] Epoch[116] Batch[0] avg_epoch_loss=2.537212\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=2.53721213341\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:58 INFO 140327940233024] Epoch[116] Batch[5] avg_epoch_loss=2.555920\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:58 INFO 140327940233024] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=2.55592012405\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:25:58 INFO 140327940233024] Epoch[116] Batch [5]#011Speed: 69.87 samples/sec#011loss=2.555920\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:05 INFO 140327940233024] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27893.59998703003, \"sum\": 27893.59998703003, \"min\": 27893.59998703003}}, \"EndTime\": 1587788765.582462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788737.688458}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:05 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5619794384 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:05 INFO 140327940233024] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=116, train loss <loss>=2.57459254265\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:05 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:16 INFO 140327940233024] Epoch[117] Batch[0] avg_epoch_loss=2.592587\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=2.59258651733\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:26 INFO 140327940233024] Epoch[117] Batch[5] avg_epoch_loss=2.556533\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:26 INFO 140327940233024] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=2.55653266112\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:26 INFO 140327940233024] Epoch[117] Batch [5]#011Speed: 67.56 samples/sec#011loss=2.556533\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:33 INFO 140327940233024] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28215.12198448181, \"sum\": 28215.12198448181, \"min\": 28215.12198448181}}, \"EndTime\": 1587788793.798107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788765.582563}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:33 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.3301299961 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:33 INFO 140327940233024] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=117, train loss <loss>=2.5418009758\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:33 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:44 INFO 140327940233024] Epoch[118] Batch[0] avg_epoch_loss=2.456332\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=2.45633172989\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:54 INFO 140327940233024] Epoch[118] Batch[5] avg_epoch_loss=2.526553\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:54 INFO 140327940233024] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=2.52655303478\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:26:54 INFO 140327940233024] Epoch[118] Batch [5]#011Speed: 68.59 samples/sec#011loss=2.526553\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:01 INFO 140327940233024] processed a total of 1214 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27871.822834014893, \"sum\": 27871.822834014893, \"min\": 27871.822834014893}}, \"EndTime\": 1587788821.670394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788793.79818}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.5563697287 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=118, train loss <loss>=2.59531033039\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:01 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:12 INFO 140327940233024] Epoch[119] Batch[0] avg_epoch_loss=2.533845\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=2.5338447094\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:22 INFO 140327940233024] Epoch[119] Batch[5] avg_epoch_loss=2.498825\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=2.49882483482\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:22 INFO 140327940233024] Epoch[119] Batch [5]#011Speed: 68.14 samples/sec#011loss=2.498825\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:29 INFO 140327940233024] processed a total of 1213 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28182.224988937378, \"sum\": 28182.224988937378, \"min\": 28182.224988937378}}, \"EndTime\": 1587788849.853163, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788821.670464}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:29 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.0411306604 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:29 INFO 140327940233024] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=119, train loss <loss>=2.50755374432\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:29 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:41 INFO 140327940233024] Epoch[120] Batch[0] avg_epoch_loss=2.527509\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=2.52750945091\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:50 INFO 140327940233024] Epoch[120] Batch[5] avg_epoch_loss=2.515998\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=2.51599780718\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:50 INFO 140327940233024] Epoch[120] Batch [5]#011Speed: 68.22 samples/sec#011loss=2.515998\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] Epoch[120] Batch[10] avg_epoch_loss=2.538870\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=2.56631669998\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] Epoch[120] Batch [10]#011Speed: 68.19 samples/sec#011loss=2.566317\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] processed a total of 1341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30115.763902664185, \"sum\": 30115.763902664185, \"min\": 30115.763902664185}}, \"EndTime\": 1587788879.969472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788849.853231}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5280123133 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=120, train loss <loss>=2.53887003118\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:27:59 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:11 INFO 140327940233024] Epoch[121] Batch[0] avg_epoch_loss=2.533490\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=2.53348994255\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:20 INFO 140327940233024] Epoch[121] Batch[5] avg_epoch_loss=2.507862\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=2.50786244869\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:20 INFO 140327940233024] Epoch[121] Batch [5]#011Speed: 68.16 samples/sec#011loss=2.507862\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] Epoch[121] Batch[10] avg_epoch_loss=2.480779\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=2.44827885628\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] Epoch[121] Batch [10]#011Speed: 68.16 samples/sec#011loss=2.448279\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30041.21494293213, \"sum\": 30041.21494293213, \"min\": 30041.21494293213}}, \"EndTime\": 1587788910.01117, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788879.969546}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.240437882 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=121, train loss <loss>=2.48077899759\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:30 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:41 INFO 140327940233024] Epoch[122] Batch[0] avg_epoch_loss=2.501838\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=2.50183820724\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:50 INFO 140327940233024] Epoch[122] Batch[5] avg_epoch_loss=2.533657\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=2.53365719318\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:50 INFO 140327940233024] Epoch[122] Batch [5]#011Speed: 69.55 samples/sec#011loss=2.533657\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] Epoch[122] Batch[10] avg_epoch_loss=2.562643\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=2.59742646217\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] Epoch[122] Batch [10]#011Speed: 70.02 samples/sec#011loss=2.597426\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29529.00195121765, \"sum\": 29529.00195121765, \"min\": 29529.00195121765}}, \"EndTime\": 1587788939.540692, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788910.011245}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.1419153243 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=122, train loss <loss>=2.56264322454\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:28:59 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:10 INFO 140327940233024] Epoch[123] Batch[0] avg_epoch_loss=2.551304\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=2.55130410194\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:20 INFO 140327940233024] Epoch[123] Batch[5] avg_epoch_loss=2.523342\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=2.52334221204\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:20 INFO 140327940233024] Epoch[123] Batch [5]#011Speed: 67.65 samples/sec#011loss=2.523342\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] Epoch[123] Batch[10] avg_epoch_loss=2.486575\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=2.44245486259\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] Epoch[123] Batch [10]#011Speed: 70.11 samples/sec#011loss=2.442455\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29703.633069992065, \"sum\": 29703.633069992065, \"min\": 29703.633069992065}}, \"EndTime\": 1587788969.244768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788939.540754}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2268827956 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=123, train loss <loss>=2.48657523502\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:29 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:40 INFO 140327940233024] Epoch[124] Batch[0] avg_epoch_loss=2.524734\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=2.52473402023\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:49 INFO 140327940233024] Epoch[124] Batch[5] avg_epoch_loss=2.522172\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=2.52217237155\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:49 INFO 140327940233024] Epoch[124] Batch [5]#011Speed: 68.92 samples/sec#011loss=2.522172\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:57 INFO 140327940233024] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27986.17911338806, \"sum\": 27986.17911338806, \"min\": 27986.17911338806}}, \"EndTime\": 1587788997.231415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788969.24484}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8434034234 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=124, train loss <loss>=2.50972952843\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:29:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:08 INFO 140327940233024] Epoch[125] Batch[0] avg_epoch_loss=2.519454\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=2.5194542408\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:17 INFO 140327940233024] Epoch[125] Batch[5] avg_epoch_loss=2.522972\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=2.52297166983\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:17 INFO 140327940233024] Epoch[125] Batch [5]#011Speed: 69.77 samples/sec#011loss=2.522972\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:24 INFO 140327940233024] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27731.43196105957, \"sum\": 27731.43196105957, \"min\": 27731.43196105957}}, \"EndTime\": 1587789024.963286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587788997.231483}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:24 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.8683703975 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:24 INFO 140327940233024] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=125, train loss <loss>=2.54119036198\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:24 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:36 INFO 140327940233024] Epoch[126] Batch[0] avg_epoch_loss=2.577536\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=2.57753634453\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:45 INFO 140327940233024] Epoch[126] Batch[5] avg_epoch_loss=2.536491\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=2.53649099668\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:45 INFO 140327940233024] Epoch[126] Batch [5]#011Speed: 70.18 samples/sec#011loss=2.536491\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:52 INFO 140327940233024] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27647.967100143433, \"sum\": 27647.967100143433, \"min\": 27647.967100143433}}, \"EndTime\": 1587789052.611876, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789024.963353}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:52 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5962526373 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:52 INFO 140327940233024] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=126, train loss <loss>=2.53985683918\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:30:52 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:03 INFO 140327940233024] Epoch[127] Batch[0] avg_epoch_loss=2.496248\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:03 INFO 140327940233024] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=2.49624752998\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:13 INFO 140327940233024] Epoch[127] Batch[5] avg_epoch_loss=2.526506\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=2.52650606632\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:13 INFO 140327940233024] Epoch[127] Batch [5]#011Speed: 68.60 samples/sec#011loss=2.526506\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] Epoch[127] Batch[10] avg_epoch_loss=2.570008\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=2.62221131325\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] Epoch[127] Batch [10]#011Speed: 67.71 samples/sec#011loss=2.622211\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30072.858095169067, \"sum\": 30072.858095169067, \"min\": 30072.858095169067}}, \"EndTime\": 1587789082.685203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789052.61194}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.8267377973 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=127, train loss <loss>=2.57000845129\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:22 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:33 INFO 140327940233024] Epoch[128] Batch[0] avg_epoch_loss=2.596240\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=2.59624004364\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:43 INFO 140327940233024] Epoch[128] Batch[5] avg_epoch_loss=2.541469\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=2.54146933556\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:43 INFO 140327940233024] Epoch[128] Batch [5]#011Speed: 69.47 samples/sec#011loss=2.541469\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:50 INFO 140327940233024] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27868.655920028687, \"sum\": 27868.655920028687, \"min\": 27868.655920028687}}, \"EndTime\": 1587789110.554364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789082.68527}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:50 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.3195522491 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:50 INFO 140327940233024] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=128, train loss <loss>=2.55241723061\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:31:50 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:01 INFO 140327940233024] Epoch[129] Batch[0] avg_epoch_loss=2.543070\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=2.54306960106\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:11 INFO 140327940233024] Epoch[129] Batch[5] avg_epoch_loss=2.510612\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=2.51061248779\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:11 INFO 140327940233024] Epoch[129] Batch [5]#011Speed: 68.33 samples/sec#011loss=2.510612\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:18 INFO 140327940233024] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28196.465015411377, \"sum\": 28196.465015411377, \"min\": 28196.465015411377}}, \"EndTime\": 1587789138.75139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789110.554437}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:18 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8636175238 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:18 INFO 140327940233024] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=129, train loss <loss>=2.48656499386\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:18 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:29 INFO 140327940233024] Epoch[130] Batch[0] avg_epoch_loss=2.646129\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=2.64612913132\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:39 INFO 140327940233024] Epoch[130] Batch[5] avg_epoch_loss=2.555116\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=2.55511554082\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:39 INFO 140327940233024] Epoch[130] Batch [5]#011Speed: 69.82 samples/sec#011loss=2.555116\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] Epoch[130] Batch[10] avg_epoch_loss=2.505044\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=2.44495716095\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] Epoch[130] Batch [10]#011Speed: 69.36 samples/sec#011loss=2.444957\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29619.494915008545, \"sum\": 29619.494915008545, \"min\": 29619.494915008545}}, \"EndTime\": 1587789168.371446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789138.751455}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6873000183 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] #quality_metric: host=algo-1, epoch=130, train loss <loss>=2.50504354997\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:48 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:59 INFO 140327940233024] Epoch[131] Batch[0] avg_epoch_loss=2.455040\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:32:59 INFO 140327940233024] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=2.45503950119\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:08 INFO 140327940233024] Epoch[131] Batch[5] avg_epoch_loss=2.492040\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=2.49203987916\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:08 INFO 140327940233024] Epoch[131] Batch [5]#011Speed: 68.15 samples/sec#011loss=2.492040\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:16 INFO 140327940233024] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28097.554922103882, \"sum\": 28097.554922103882, \"min\": 28097.554922103882}}, \"EndTime\": 1587789196.469457, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789168.371509}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:16 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.9859253611 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:16 INFO 140327940233024] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:16 INFO 140327940233024] #quality_metric: host=algo-1, epoch=131, train loss <loss>=2.48708891869\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:16 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:27 INFO 140327940233024] Epoch[132] Batch[0] avg_epoch_loss=2.563535\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=2.56353545189\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:37 INFO 140327940233024] Epoch[132] Batch[5] avg_epoch_loss=2.497518\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:37 INFO 140327940233024] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=2.49751750628\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:37 INFO 140327940233024] Epoch[132] Batch [5]#011Speed: 69.87 samples/sec#011loss=2.497518\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] Epoch[132] Batch[10] avg_epoch_loss=2.464585\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=2.42506594658\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] Epoch[132] Batch [10]#011Speed: 69.36 samples/sec#011loss=2.425066\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29762.174129486084, \"sum\": 29762.174129486084, \"min\": 29762.174129486084}}, \"EndTime\": 1587789226.232168, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789196.46954}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2762672184 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=132, train loss <loss>=2.46458497914\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:46 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:57 INFO 140327940233024] Epoch[133] Batch[0] avg_epoch_loss=2.448636\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:33:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=2.44863581657\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:06 INFO 140327940233024] Epoch[133] Batch[5] avg_epoch_loss=2.516792\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=2.51679213842\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:06 INFO 140327940233024] Epoch[133] Batch [5]#011Speed: 68.71 samples/sec#011loss=2.516792\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:14 INFO 140327940233024] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27927.947998046875, \"sum\": 27927.947998046875, \"min\": 27927.947998046875}}, \"EndTime\": 1587789254.160522, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789226.232232}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:14 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.3640060077 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:14 INFO 140327940233024] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=133, train loss <loss>=2.48351562023\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:14 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:25 INFO 140327940233024] Epoch[134] Batch[0] avg_epoch_loss=2.485335\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=2.48533535004\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:34 INFO 140327940233024] Epoch[134] Batch[5] avg_epoch_loss=2.526069\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=2.52606852849\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:34 INFO 140327940233024] Epoch[134] Batch [5]#011Speed: 68.85 samples/sec#011loss=2.526069\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] Epoch[134] Batch[10] avg_epoch_loss=2.460013\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=2.38074579239\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] Epoch[134] Batch [10]#011Speed: 68.87 samples/sec#011loss=2.380746\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29947.20506668091, \"sum\": 29947.20506668091, \"min\": 29947.20506668091}}, \"EndTime\": 1587789284.108254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789254.160586}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.8085320145 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=134, train loss <loss>=2.46001273935\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:44 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:55 INFO 140327940233024] Epoch[135] Batch[0] avg_epoch_loss=2.566122\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:34:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=2.56612205505\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:04 INFO 140327940233024] Epoch[135] Batch[5] avg_epoch_loss=2.530361\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=2.53036097685\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:04 INFO 140327940233024] Epoch[135] Batch [5]#011Speed: 69.01 samples/sec#011loss=2.530361\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] Epoch[135] Batch[10] avg_epoch_loss=2.474232\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=2.40687768459\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] Epoch[135] Batch [10]#011Speed: 67.58 samples/sec#011loss=2.406878\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] processed a total of 1338 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30005.752086639404, \"sum\": 30005.752086639404, \"min\": 30005.752086639404}}, \"EndTime\": 1587789314.114367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789284.10832}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5913120222 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=135, train loss <loss>=2.47423220765\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:14 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:25 INFO 140327940233024] Epoch[136] Batch[0] avg_epoch_loss=2.538745\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=2.53874516487\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:34 INFO 140327940233024] Epoch[136] Batch[5] avg_epoch_loss=2.508928\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=2.50892798106\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:34 INFO 140327940233024] Epoch[136] Batch [5]#011Speed: 69.39 samples/sec#011loss=2.508928\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:41 INFO 140327940233024] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27757.40694999695, \"sum\": 27757.40694999695, \"min\": 27757.40694999695}}, \"EndTime\": 1587789341.872231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789314.114431}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:41 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.2490070279 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:41 INFO 140327940233024] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=136, train loss <loss>=2.54822533131\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:41 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:52 INFO 140327940233024] Epoch[137] Batch[0] avg_epoch_loss=2.502882\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:35:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=2.50288248062\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:02 INFO 140327940233024] Epoch[137] Batch[5] avg_epoch_loss=2.500301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=2.50030088425\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:02 INFO 140327940233024] Epoch[137] Batch [5]#011Speed: 69.71 samples/sec#011loss=2.500301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] Epoch[137] Batch[10] avg_epoch_loss=2.368705\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=2.21079028845\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] Epoch[137] Batch [10]#011Speed: 68.91 samples/sec#011loss=2.210790\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29591.815948486328, \"sum\": 29591.815948486328, \"min\": 29591.815948486328}}, \"EndTime\": 1587789371.464476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789341.872305}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.2888407202 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=137, train loss <loss>=2.36870515888\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:11 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/state_b77230af-3383-4a80-bbcd-bb935f95e9df-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 208.22691917419434, \"sum\": 208.22691917419434, \"min\": 208.22691917419434}}, \"EndTime\": 1587789371.673255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789371.464548}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:22 INFO 140327940233024] Epoch[138] Batch[0] avg_epoch_loss=2.466110\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=2.46610999107\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:32 INFO 140327940233024] Epoch[138] Batch[5] avg_epoch_loss=2.487602\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=2.48760199547\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:32 INFO 140327940233024] Epoch[138] Batch [5]#011Speed: 68.56 samples/sec#011loss=2.487602\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] Epoch[138] Batch[10] avg_epoch_loss=2.385736\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=2.26349768639\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] Epoch[138] Batch [10]#011Speed: 68.37 samples/sec#011loss=2.263498\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29884.95397567749, \"sum\": 29884.95397567749, \"min\": 29884.95397567749}}, \"EndTime\": 1587789401.558337, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789371.673324}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.1319323993 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=138, train loss <loss>=2.38573640043\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:41 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:53 INFO 140327940233024] Epoch[139] Batch[0] avg_epoch_loss=2.526882\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:36:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=2.52688217163\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:02 INFO 140327940233024] Epoch[139] Batch[5] avg_epoch_loss=2.524342\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=2.52434209983\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:02 INFO 140327940233024] Epoch[139] Batch [5]#011Speed: 69.29 samples/sec#011loss=2.524342\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] Epoch[139] Batch[10] avg_epoch_loss=2.471106\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=2.40722203255\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] Epoch[139] Batch [10]#011Speed: 69.59 samples/sec#011loss=2.407222\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29875.756978988647, \"sum\": 29875.756978988647, \"min\": 29875.756978988647}}, \"EndTime\": 1587789431.434448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789401.558403}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.9778522926 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=139, train loss <loss>=2.47110570561\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:11 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:22 INFO 140327940233024] Epoch[140] Batch[0] avg_epoch_loss=2.464427\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=2.46442699432\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:32 INFO 140327940233024] Epoch[140] Batch[5] avg_epoch_loss=2.484483\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=2.48448264599\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:32 INFO 140327940233024] Epoch[140] Batch [5]#011Speed: 68.30 samples/sec#011loss=2.484483\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] Epoch[140] Batch[10] avg_epoch_loss=2.513450\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=2.54821066856\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] Epoch[140] Batch [10]#011Speed: 68.33 samples/sec#011loss=2.548211\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30015.246868133545, \"sum\": 30015.246868133545, \"min\": 30015.246868133545}}, \"EndTime\": 1587789461.450145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789431.434514}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.8780564908 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=140, train loss <loss>=2.51344992898\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:41 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:52 INFO 140327940233024] Epoch[141] Batch[0] avg_epoch_loss=2.535358\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:37:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=2.5353577137\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:02 INFO 140327940233024] Epoch[141] Batch[5] avg_epoch_loss=2.564945\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=2.56494526068\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:02 INFO 140327940233024] Epoch[141] Batch [5]#011Speed: 68.22 samples/sec#011loss=2.564945\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] Epoch[141] Batch[10] avg_epoch_loss=2.582144\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=2.60278215408\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] Epoch[141] Batch [10]#011Speed: 67.82 samples/sec#011loss=2.602782\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30145.15709877014, \"sum\": 30145.15709877014, \"min\": 30145.15709877014}}, \"EndTime\": 1587789491.595774, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789461.450216}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6884358031 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] #quality_metric: host=algo-1, epoch=141, train loss <loss>=2.58214384859\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:11 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:22 INFO 140327940233024] Epoch[142] Batch[0] avg_epoch_loss=2.523792\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=2.52379179001\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:32 INFO 140327940233024] Epoch[142] Batch[5] avg_epoch_loss=2.484411\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=2.48441060384\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:32 INFO 140327940233024] Epoch[142] Batch [5]#011Speed: 68.69 samples/sec#011loss=2.484411\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:39 INFO 140327940233024] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28086.498975753784, \"sum\": 28086.498975753784, \"min\": 28086.498975753784}}, \"EndTime\": 1587789519.682775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789491.595859}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.5377327226 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=142, train loss <loss>=2.49059228897\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:39 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:50 INFO 140327940233024] Epoch[143] Batch[0] avg_epoch_loss=2.565398\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:38:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=2.56539797783\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:00 INFO 140327940233024] Epoch[143] Batch[5] avg_epoch_loss=2.543823\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=2.54382272561\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:00 INFO 140327940233024] Epoch[143] Batch [5]#011Speed: 69.74 samples/sec#011loss=2.543823\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:07 INFO 140327940233024] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27765.14506340027, \"sum\": 27765.14506340027, \"min\": 27765.14506340027}}, \"EndTime\": 1587789547.448476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789519.682841}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:07 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2999983625 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:07 INFO 140327940233024] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:07 INFO 140327940233024] #quality_metric: host=algo-1, epoch=143, train loss <loss>=2.53700039387\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:07 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:18 INFO 140327940233024] Epoch[144] Batch[0] avg_epoch_loss=2.472317\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=2.4723174572\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:28 INFO 140327940233024] Epoch[144] Batch[5] avg_epoch_loss=2.520870\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:28 INFO 140327940233024] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=2.52086961269\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:28 INFO 140327940233024] Epoch[144] Batch [5]#011Speed: 68.29 samples/sec#011loss=2.520870\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:35 INFO 140327940233024] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28174.968004226685, \"sum\": 28174.968004226685, \"min\": 28174.968004226685}}, \"EndTime\": 1587789575.623908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789547.448543}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:35 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2589864058 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:35 INFO 140327940233024] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:35 INFO 140327940233024] #quality_metric: host=algo-1, epoch=144, train loss <loss>=2.4886223793\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:35 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:46 INFO 140327940233024] Epoch[145] Batch[0] avg_epoch_loss=2.460289\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=2.46028852463\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:56 INFO 140327940233024] Epoch[145] Batch[5] avg_epoch_loss=2.499803\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=2.49980274836\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:39:56 INFO 140327940233024] Epoch[145] Batch [5]#011Speed: 68.03 samples/sec#011loss=2.499803\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] Epoch[145] Batch[10] avg_epoch_loss=2.521796\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=2.54818716049\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] Epoch[145] Batch [10]#011Speed: 68.12 samples/sec#011loss=2.548187\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30171.30994796753, \"sum\": 30171.30994796753, \"min\": 30171.30994796753}}, \"EndTime\": 1587789605.795641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789575.623976}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.8220060879 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] #quality_metric: host=algo-1, epoch=145, train loss <loss>=2.52179566297\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:05 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:17 INFO 140327940233024] Epoch[146] Batch[0] avg_epoch_loss=2.607740\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=2.60773968697\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:26 INFO 140327940233024] Epoch[146] Batch[5] avg_epoch_loss=2.531017\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:26 INFO 140327940233024] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=2.53101682663\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:26 INFO 140327940233024] Epoch[146] Batch [5]#011Speed: 67.95 samples/sec#011loss=2.531017\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] Epoch[146] Batch[10] avg_epoch_loss=2.582216\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=2.64365592003\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] Epoch[146] Batch [10]#011Speed: 68.22 samples/sec#011loss=2.643656\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] processed a total of 1347 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30147.63903617859, \"sum\": 30147.63903617859, \"min\": 30147.63903617859}}, \"EndTime\": 1587789635.943725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789605.795704}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6799485604 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] #quality_metric: host=algo-1, epoch=146, train loss <loss>=2.58221641454\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:35 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:47 INFO 140327940233024] Epoch[147] Batch[0] avg_epoch_loss=2.648508\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:47 INFO 140327940233024] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=2.6485080719\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:56 INFO 140327940233024] Epoch[147] Batch[5] avg_epoch_loss=2.525061\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=2.52506057421\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:40:56 INFO 140327940233024] Epoch[147] Batch [5]#011Speed: 69.28 samples/sec#011loss=2.525061\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:03 INFO 140327940233024] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27918.209075927734, \"sum\": 27918.209075927734, \"min\": 27918.209075927734}}, \"EndTime\": 1587789663.862463, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789635.943806}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:03 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.7764048562 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:03 INFO 140327940233024] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:03 INFO 140327940233024] #quality_metric: host=algo-1, epoch=147, train loss <loss>=2.51658895016\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:03 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:14 INFO 140327940233024] Epoch[148] Batch[0] avg_epoch_loss=2.385167\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=2.38516664505\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:24 INFO 140327940233024] Epoch[148] Batch[5] avg_epoch_loss=2.472249\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=2.47224934896\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:24 INFO 140327940233024] Epoch[148] Batch [5]#011Speed: 68.86 samples/sec#011loss=2.472249\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] Epoch[148] Batch[10] avg_epoch_loss=2.577161\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=2.70305528641\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] Epoch[148] Batch [10]#011Speed: 69.42 samples/sec#011loss=2.703055\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29594.2440032959, \"sum\": 29594.2440032959, \"min\": 29594.2440032959}}, \"EndTime\": 1587789693.457144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789663.862523}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.4541877632 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] #quality_metric: host=algo-1, epoch=148, train loss <loss>=2.57716113871\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:33 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:44 INFO 140327940233024] Epoch[149] Batch[0] avg_epoch_loss=2.420247\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:44 INFO 140327940233024] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=2.42024731636\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:53 INFO 140327940233024] Epoch[149] Batch[5] avg_epoch_loss=2.490657\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=2.49065748851\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:41:53 INFO 140327940233024] Epoch[149] Batch [5]#011Speed: 68.40 samples/sec#011loss=2.490657\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:01 INFO 140327940233024] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28005.326986312866, \"sum\": 28005.326986312866, \"min\": 28005.326986312866}}, \"EndTime\": 1587789721.462951, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789693.457253}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5984954805 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=149, train loss <loss>=2.52981705666\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:01 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:12 INFO 140327940233024] Epoch[150] Batch[0] avg_epoch_loss=2.548730\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=2.54872965813\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:22 INFO 140327940233024] Epoch[150] Batch[5] avg_epoch_loss=2.492126\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:22 INFO 140327940233024] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=2.49212614695\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:22 INFO 140327940233024] Epoch[150] Batch [5]#011Speed: 68.84 samples/sec#011loss=2.492126\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] Epoch[150] Batch[10] avg_epoch_loss=2.499905\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=2.50923914909\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] Epoch[150] Batch [10]#011Speed: 67.38 samples/sec#011loss=2.509239\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30124.732971191406, \"sum\": 30124.732971191406, \"min\": 30124.732971191406}}, \"EndTime\": 1587789751.588177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789721.463022}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2160157418 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=150, train loss <loss>=2.49990478429\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:31 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:42 INFO 140327940233024] Epoch[151] Batch[0] avg_epoch_loss=2.434022\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=2.43402194977\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:51 INFO 140327940233024] Epoch[151] Batch[5] avg_epoch_loss=2.463788\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=2.46378787359\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:42:51 INFO 140327940233024] Epoch[151] Batch [5]#011Speed: 69.70 samples/sec#011loss=2.463788\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] Epoch[151] Batch[10] avg_epoch_loss=2.456164\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=2.44701480865\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] Epoch[151] Batch [10]#011Speed: 69.14 samples/sec#011loss=2.447015\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] processed a total of 1350 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29655.65299987793, \"sum\": 29655.65299987793, \"min\": 29655.65299987793}}, \"EndTime\": 1587789781.244278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789751.588244}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.5223771823 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=151, train loss <loss>=2.45616375316\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:01 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:12 INFO 140327940233024] Epoch[152] Batch[0] avg_epoch_loss=2.506336\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=2.50633597374\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:21 INFO 140327940233024] Epoch[152] Batch[5] avg_epoch_loss=2.487698\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=2.48769827684\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:21 INFO 140327940233024] Epoch[152] Batch [5]#011Speed: 68.36 samples/sec#011loss=2.487698\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:29 INFO 140327940233024] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28179.636001586914, \"sum\": 28179.636001586914, \"min\": 28179.636001586914}}, \"EndTime\": 1587789809.424346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789781.24434}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:29 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.6483752797 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:29 INFO 140327940233024] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=152, train loss <loss>=2.49183282852\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:29 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:40 INFO 140327940233024] Epoch[153] Batch[0] avg_epoch_loss=2.548886\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=2.54888629913\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:49 INFO 140327940233024] Epoch[153] Batch[5] avg_epoch_loss=2.536538\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=2.53653840224\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:49 INFO 140327940233024] Epoch[153] Batch [5]#011Speed: 69.33 samples/sec#011loss=2.536538\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:57 INFO 140327940233024] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28055.693864822388, \"sum\": 28055.693864822388, \"min\": 28055.693864822388}}, \"EndTime\": 1587789837.480556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789809.424417}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6610016294 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=153, train loss <loss>=2.55154798031\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:43:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:08 INFO 140327940233024] Epoch[154] Batch[0] avg_epoch_loss=2.468221\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:08 INFO 140327940233024] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=2.46822118759\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:17 INFO 140327940233024] Epoch[154] Batch[5] avg_epoch_loss=2.491752\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=2.4917516311\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:17 INFO 140327940233024] Epoch[154] Batch [5]#011Speed: 70.26 samples/sec#011loss=2.491752\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:25 INFO 140327940233024] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27821.5548992157, \"sum\": 27821.5548992157, \"min\": 27821.5548992157}}, \"EndTime\": 1587789865.302702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789837.480626}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:25 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2820244383 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:25 INFO 140327940233024] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=154, train loss <loss>=2.49964692593\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:25 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:36 INFO 140327940233024] Epoch[155] Batch[0] avg_epoch_loss=2.468047\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=2.46804690361\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:45 INFO 140327940233024] Epoch[155] Batch[5] avg_epoch_loss=2.481814\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:45 INFO 140327940233024] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=2.48181358973\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:45 INFO 140327940233024] Epoch[155] Batch [5]#011Speed: 70.10 samples/sec#011loss=2.481814\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:52 INFO 140327940233024] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27536.03982925415, \"sum\": 27536.03982925415, \"min\": 27536.03982925415}}, \"EndTime\": 1587789892.839223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789865.302784}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:52 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6685798302 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:52 INFO 140327940233024] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=155, train loss <loss>=2.48009238243\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:44:52 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:04 INFO 140327940233024] Epoch[156] Batch[0] avg_epoch_loss=2.501410\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=2.50141048431\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:13 INFO 140327940233024] Epoch[156] Batch[5] avg_epoch_loss=2.534301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=2.53430124124\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:13 INFO 140327940233024] Epoch[156] Batch [5]#011Speed: 69.51 samples/sec#011loss=2.534301\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:20 INFO 140327940233024] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27959.47813987732, \"sum\": 27959.47813987732, \"min\": 27959.47813987732}}, \"EndTime\": 1587789920.799229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789892.839288}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:20 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8862301082 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:20 INFO 140327940233024] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=156, train loss <loss>=2.52840361595\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:20 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:32 INFO 140327940233024] Epoch[157] Batch[0] avg_epoch_loss=2.608742\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=2.60874199867\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:41 INFO 140327940233024] Epoch[157] Batch[5] avg_epoch_loss=2.529401\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:41 INFO 140327940233024] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=2.52940090497\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:41 INFO 140327940233024] Epoch[157] Batch [5]#011Speed: 69.64 samples/sec#011loss=2.529401\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] Epoch[157] Batch[10] avg_epoch_loss=2.533514\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=2.53845000267\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] Epoch[157] Batch [10]#011Speed: 69.45 samples/sec#011loss=2.538450\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29661.84902191162, \"sum\": 29661.84902191162, \"min\": 29661.84902191162}}, \"EndTime\": 1587789950.461655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789920.799296}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.5688985832 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=157, train loss <loss>=2.5335141312\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:45:50 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:01 INFO 140327940233024] Epoch[158] Batch[0] avg_epoch_loss=2.543822\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:01 INFO 140327940233024] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=2.54382157326\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:10 INFO 140327940233024] Epoch[158] Batch[5] avg_epoch_loss=2.511187\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:10 INFO 140327940233024] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=2.51118731499\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:10 INFO 140327940233024] Epoch[158] Batch [5]#011Speed: 69.57 samples/sec#011loss=2.511187\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:18 INFO 140327940233024] processed a total of 1221 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27784.805059432983, \"sum\": 27784.805059432983, \"min\": 27784.805059432983}}, \"EndTime\": 1587789978.246904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789950.461715}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:18 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.944707385 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:18 INFO 140327940233024] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=158, train loss <loss>=2.5805737257\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:18 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:29 INFO 140327940233024] Epoch[159] Batch[0] avg_epoch_loss=2.557583\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=2.55758309364\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:38 INFO 140327940233024] Epoch[159] Batch[5] avg_epoch_loss=2.517728\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:38 INFO 140327940233024] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=2.51772753398\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:38 INFO 140327940233024] Epoch[159] Batch [5]#011Speed: 68.60 samples/sec#011loss=2.517728\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:46 INFO 140327940233024] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28243.91198158264, \"sum\": 28243.91198158264, \"min\": 28243.91198158264}}, \"EndTime\": 1587790006.491381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587789978.246976}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:46 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2571403604 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:46 INFO 140327940233024] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:46 INFO 140327940233024] #quality_metric: host=algo-1, epoch=159, train loss <loss>=2.54803168774\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:46 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:57 INFO 140327940233024] Epoch[160] Batch[0] avg_epoch_loss=2.439146\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:46:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=2.43914556503\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:07 INFO 140327940233024] Epoch[160] Batch[5] avg_epoch_loss=2.487633\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:07 INFO 140327940233024] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=2.48763306936\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:07 INFO 140327940233024] Epoch[160] Batch [5]#011Speed: 69.38 samples/sec#011loss=2.487633\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:14 INFO 140327940233024] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28009.62495803833, \"sum\": 28009.62495803833, \"min\": 28009.62495803833}}, \"EndTime\": 1587790034.501624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790006.49146}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:14 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8771758023 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:14 INFO 140327940233024] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:14 INFO 140327940233024] #quality_metric: host=algo-1, epoch=160, train loss <loss>=2.48777222633\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:14 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:25 INFO 140327940233024] Epoch[161] Batch[0] avg_epoch_loss=2.605519\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:25 INFO 140327940233024] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=2.6055188179\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:34 INFO 140327940233024] Epoch[161] Batch[5] avg_epoch_loss=2.528444\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:34 INFO 140327940233024] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=2.52844365438\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:34 INFO 140327940233024] Epoch[161] Batch [5]#011Speed: 68.06 samples/sec#011loss=2.528444\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:42 INFO 140327940233024] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27511.780977249146, \"sum\": 27511.780977249146, \"min\": 27511.780977249146}}, \"EndTime\": 1587790062.0139, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790034.501746}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:42 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=46.2345317846 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:42 INFO 140327940233024] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:42 INFO 140327940233024] #quality_metric: host=algo-1, epoch=161, train loss <loss>=2.53939404488\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:42 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:53 INFO 140327940233024] Epoch[162] Batch[0] avg_epoch_loss=2.493975\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:47:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=2.49397516251\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:02 INFO 140327940233024] Epoch[162] Batch[5] avg_epoch_loss=2.490068\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=2.49006787936\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:02 INFO 140327940233024] Epoch[162] Batch [5]#011Speed: 69.11 samples/sec#011loss=2.490068\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:09 INFO 140327940233024] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27879.6169757843, \"sum\": 27879.6169757843, \"min\": 27879.6169757843}}, \"EndTime\": 1587790089.894049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790062.013987}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:09 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.8397815502 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:09 INFO 140327940233024] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=162, train loss <loss>=2.49221408367\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:09 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:21 INFO 140327940233024] Epoch[163] Batch[0] avg_epoch_loss=2.460251\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=2.46025061607\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:30 INFO 140327940233024] Epoch[163] Batch[5] avg_epoch_loss=2.454133\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=2.45413335164\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:30 INFO 140327940233024] Epoch[163] Batch [5]#011Speed: 68.16 samples/sec#011loss=2.454133\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] Epoch[163] Batch[10] avg_epoch_loss=2.495010\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=2.54406242371\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] Epoch[163] Batch [10]#011Speed: 67.90 samples/sec#011loss=2.544062\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] processed a total of 1338 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30090.978145599365, \"sum\": 30090.978145599365, \"min\": 30090.978145599365}}, \"EndTime\": 1587790119.985465, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790089.894116}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4650085482 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] #quality_metric: host=algo-1, epoch=163, train loss <loss>=2.49501020258\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:39 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:51 INFO 140327940233024] Epoch[164] Batch[0] avg_epoch_loss=2.546640\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:48:51 INFO 140327940233024] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=2.54664039612\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:00 INFO 140327940233024] Epoch[164] Batch[5] avg_epoch_loss=2.534771\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=2.5347713232\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:00 INFO 140327940233024] Epoch[164] Batch [5]#011Speed: 68.93 samples/sec#011loss=2.534771\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] Epoch[164] Batch[10] avg_epoch_loss=2.577915\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=2.62968659401\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] Epoch[164] Batch [10]#011Speed: 69.60 samples/sec#011loss=2.629687\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29797.93405532837, \"sum\": 29797.93405532837, \"min\": 29797.93405532837}}, \"EndTime\": 1587790149.783757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790119.985533}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4989841274 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=164, train loss <loss>=2.57791462812\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:09 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:20 INFO 140327940233024] Epoch[165] Batch[0] avg_epoch_loss=2.521474\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:20 INFO 140327940233024] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=2.52147388458\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:30 INFO 140327940233024] Epoch[165] Batch[5] avg_epoch_loss=2.501964\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:30 INFO 140327940233024] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=2.50196433067\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:30 INFO 140327940233024] Epoch[165] Batch [5]#011Speed: 68.63 samples/sec#011loss=2.501964\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:37 INFO 140327940233024] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27979.61711883545, \"sum\": 27979.61711883545, \"min\": 27979.61711883545}}, \"EndTime\": 1587790177.765188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790149.784167}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:37 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.6044672019 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:37 INFO 140327940233024] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:37 INFO 140327940233024] #quality_metric: host=algo-1, epoch=165, train loss <loss>=2.51068820953\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:37 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:49 INFO 140327940233024] Epoch[166] Batch[0] avg_epoch_loss=2.411635\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:49 INFO 140327940233024] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=2.41163516045\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:58 INFO 140327940233024] Epoch[166] Batch[5] avg_epoch_loss=2.473925\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:58 INFO 140327940233024] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=2.47392523289\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:49:58 INFO 140327940233024] Epoch[166] Batch [5]#011Speed: 68.71 samples/sec#011loss=2.473925\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:06 INFO 140327940233024] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28393.09787750244, \"sum\": 28393.09787750244, \"min\": 28393.09787750244}}, \"EndTime\": 1587790206.158873, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790177.765253}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:06 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.3906758636 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:06 INFO 140327940233024] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=166, train loss <loss>=2.48115181923\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:06 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:17 INFO 140327940233024] Epoch[167] Batch[0] avg_epoch_loss=2.440932\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:17 INFO 140327940233024] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=2.44093155861\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:26 INFO 140327940233024] Epoch[167] Batch[5] avg_epoch_loss=2.490586\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:26 INFO 140327940233024] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=2.49058640003\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:26 INFO 140327940233024] Epoch[167] Batch [5]#011Speed: 69.40 samples/sec#011loss=2.490586\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] Epoch[167] Batch[10] avg_epoch_loss=2.474515\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=2.45522880554\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] Epoch[167] Batch [10]#011Speed: 68.61 samples/sec#011loss=2.455229\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29948.594093322754, \"sum\": 29948.594093322754, \"min\": 29948.594093322754}}, \"EndTime\": 1587790236.107999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790206.15894}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.1404423789 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] #quality_metric: host=algo-1, epoch=167, train loss <loss>=2.47451476617\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:36 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:47 INFO 140327940233024] Epoch[168] Batch[0] avg_epoch_loss=2.463463\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:47 INFO 140327940233024] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=2.46346330643\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:56 INFO 140327940233024] Epoch[168] Batch[5] avg_epoch_loss=2.503237\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:56 INFO 140327940233024] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=2.50323657195\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:50:56 INFO 140327940233024] Epoch[168] Batch [5]#011Speed: 70.07 samples/sec#011loss=2.503237\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:04 INFO 140327940233024] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27973.56104850769, \"sum\": 27973.56104850769, \"min\": 27973.56104850769}}, \"EndTime\": 1587790264.082033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790236.108066}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:04 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.0771768266 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:04 INFO 140327940233024] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:04 INFO 140327940233024] #quality_metric: host=algo-1, epoch=168, train loss <loss>=2.49872713089\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:04 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:15 INFO 140327940233024] Epoch[169] Batch[0] avg_epoch_loss=2.538758\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=2.53875803947\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:24 INFO 140327940233024] Epoch[169] Batch[5] avg_epoch_loss=2.479762\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:24 INFO 140327940233024] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=2.47976183891\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:24 INFO 140327940233024] Epoch[169] Batch [5]#011Speed: 69.58 samples/sec#011loss=2.479762\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:31 INFO 140327940233024] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27895.319938659668, \"sum\": 27895.319938659668, \"min\": 27895.319938659668}}, \"EndTime\": 1587790291.977907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790264.082094}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:31 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=45.5988601419 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:31 INFO 140327940233024] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:31 INFO 140327940233024] #quality_metric: host=algo-1, epoch=169, train loss <loss>=2.46163096428\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:31 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:43 INFO 140327940233024] Epoch[170] Batch[0] avg_epoch_loss=2.350163\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=2.35016274452\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:52 INFO 140327940233024] Epoch[170] Batch[5] avg_epoch_loss=2.470366\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:52 INFO 140327940233024] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=2.47036623955\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:51:52 INFO 140327940233024] Epoch[170] Batch [5]#011Speed: 68.38 samples/sec#011loss=2.470366\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] Epoch[170] Batch[10] avg_epoch_loss=2.371676\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=2.25324811935\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] Epoch[170] Batch [10]#011Speed: 65.55 samples/sec#011loss=2.253248\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30465.135097503662, \"sum\": 30465.135097503662, \"min\": 30465.135097503662}}, \"EndTime\": 1587790322.443561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790291.977981}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=42.3105277772 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] #quality_metric: host=algo-1, epoch=170, train loss <loss>=2.37167618491\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:02 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:13 INFO 140327940233024] Epoch[171] Batch[0] avg_epoch_loss=2.457811\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:13 INFO 140327940233024] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=2.45781111717\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:23 INFO 140327940233024] Epoch[171] Batch[5] avg_epoch_loss=2.489063\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=2.48906286558\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:23 INFO 140327940233024] Epoch[171] Batch [5]#011Speed: 68.29 samples/sec#011loss=2.489063\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] Epoch[171] Batch[10] avg_epoch_loss=2.451748\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=2.40696964264\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] Epoch[171] Batch [10]#011Speed: 68.06 samples/sec#011loss=2.406970\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 30172.28889465332, \"sum\": 30172.28889465332, \"min\": 30172.28889465332}}, \"EndTime\": 1587790352.616305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790322.443627}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=43.1851854656 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] #quality_metric: host=algo-1, epoch=171, train loss <loss>=2.45174776424\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:32 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:43 INFO 140327940233024] Epoch[172] Batch[0] avg_epoch_loss=2.490901\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:43 INFO 140327940233024] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=2.49090051651\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:53 INFO 140327940233024] Epoch[172] Batch[5] avg_epoch_loss=2.496234\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:53 INFO 140327940233024] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=2.49623397986\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:52:53 INFO 140327940233024] Epoch[172] Batch [5]#011Speed: 69.41 samples/sec#011loss=2.496234\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:00 INFO 140327940233024] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28028.679132461548, \"sum\": 28028.679132461548, \"min\": 28028.679132461548}}, \"EndTime\": 1587790380.645445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790352.616365}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:00 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.2045783376 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:00 INFO 140327940233024] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:00 INFO 140327940233024] #quality_metric: host=algo-1, epoch=172, train loss <loss>=2.5149228096\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:00 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:12 INFO 140327940233024] Epoch[173] Batch[0] avg_epoch_loss=2.454287\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:12 INFO 140327940233024] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=2.45428729057\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:21 INFO 140327940233024] Epoch[173] Batch[5] avg_epoch_loss=2.491773\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:21 INFO 140327940233024] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=2.49177292983\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:21 INFO 140327940233024] Epoch[173] Batch [5]#011Speed: 68.15 samples/sec#011loss=2.491773\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:29 INFO 140327940233024] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28477.829933166504, \"sum\": 28477.829933166504, \"min\": 28477.829933166504}}, \"EndTime\": 1587790409.123866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790380.64551}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:29 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.4553520526 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:29 INFO 140327940233024] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:29 INFO 140327940233024] #quality_metric: host=algo-1, epoch=173, train loss <loss>=2.48000054359\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:29 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:40 INFO 140327940233024] Epoch[174] Batch[0] avg_epoch_loss=2.530004\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:40 INFO 140327940233024] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=2.53000378609\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:50 INFO 140327940233024] Epoch[174] Batch[5] avg_epoch_loss=2.493107\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:50 INFO 140327940233024] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=2.49310735861\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:50 INFO 140327940233024] Epoch[174] Batch [5]#011Speed: 67.49 samples/sec#011loss=2.493107\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:57 INFO 140327940233024] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 28608.7589263916, \"sum\": 28608.7589263916, \"min\": 28608.7589263916}}, \"EndTime\": 1587790437.733242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790409.124005}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:57 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.636493769 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:57 INFO 140327940233024] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:57 INFO 140327940233024] #quality_metric: host=algo-1, epoch=174, train loss <loss>=2.48209395409\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:53:57 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:09 INFO 140327940233024] Epoch[175] Batch[0] avg_epoch_loss=2.443298\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:09 INFO 140327940233024] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=2.44329810143\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:18 INFO 140327940233024] Epoch[175] Batch[5] avg_epoch_loss=2.531123\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:18 INFO 140327940233024] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=2.53112336\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:18 INFO 140327940233024] Epoch[175] Batch [5]#011Speed: 70.15 samples/sec#011loss=2.531123\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] Epoch[175] Batch[10] avg_epoch_loss=2.540863\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=2.55254950523\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] Epoch[175] Batch [10]#011Speed: 69.90 samples/sec#011loss=2.552550\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 29659.99412536621, \"sum\": 29659.99412536621, \"min\": 29659.99412536621}}, \"EndTime\": 1587790467.393685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790437.733327}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.6053757417 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] #quality_metric: host=algo-1, epoch=175, train loss <loss>=2.54086251692\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:27 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:38 INFO 140327940233024] Epoch[176] Batch[0] avg_epoch_loss=2.491293\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:38 INFO 140327940233024] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=2.49129295349\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:47 INFO 140327940233024] Epoch[176] Batch[5] avg_epoch_loss=2.502733\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:47 INFO 140327940233024] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=2.50273283323\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:47 INFO 140327940233024] Epoch[176] Batch [5]#011Speed: 68.88 samples/sec#011loss=2.502733\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:55 INFO 140327940233024] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27932.196140289307, \"sum\": 27932.196140289307, \"min\": 27932.196140289307}}, \"EndTime\": 1587790495.326418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790467.393757}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:55 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.8226588281 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:55 INFO 140327940233024] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:55 INFO 140327940233024] #quality_metric: host=algo-1, epoch=176, train loss <loss>=2.48267512321\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:54:55 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:06 INFO 140327940233024] Epoch[177] Batch[0] avg_epoch_loss=2.420254\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:06 INFO 140327940233024] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=2.42025399208\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:15 INFO 140327940233024] Epoch[177] Batch[5] avg_epoch_loss=2.468925\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:15 INFO 140327940233024] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=2.46892460187\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:15 INFO 140327940233024] Epoch[177] Batch [5]#011Speed: 69.90 samples/sec#011loss=2.468925\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27988.018035888672, \"sum\": 27988.018035888672, \"min\": 27988.018035888672}}, \"EndTime\": 1587790523.314974, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790495.326483}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] #throughput_metric: host=algo-1, train throughput=44.9833361354 records/second\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] #quality_metric: host=algo-1, epoch=177, train loss <loss>=2.47180447578\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] Loading parameters from best epoch (137)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 87.85295486450195, \"sum\": 87.85295486450195, \"min\": 87.85295486450195}}, \"EndTime\": 1587790523.40358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790523.315067}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] stopping training now\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] Final loss: 2.36870515888 (occurred at epoch 137)\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] #quality_metric: host=algo-1, train final_loss <loss>=2.36870515888\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 WARNING 140327940233024] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 WARNING 140327940233024] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:23 INFO 140327940233024] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 32525.93183517456, \"sum\": 32525.93183517456, \"min\": 32525.93183517456}}, \"EndTime\": 1587790555.930299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790523.403657}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:56 INFO 140327940233024] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 33449.79214668274, \"sum\": 33449.79214668274, \"min\": 33449.79214668274}}, \"EndTime\": 1587790556.85412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790555.931506}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:56 INFO 140327940233024] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:56 INFO 140327940233024] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 135.59198379516602, \"sum\": 135.59198379516602, \"min\": 135.59198379516602}}, \"EndTime\": 1587790556.989812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790556.854177}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:56 INFO 140327940233024] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:55:56 INFO 140327940233024] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03600120544433594, \"sum\": 0.03600120544433594, \"min\": 0.03600120544433594}}, \"EndTime\": 1587790556.990585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790556.989864}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 33406.75592422485, \"sum\": 33406.75592422485, \"min\": 33406.75592422485}}, \"EndTime\": 1587790590.397312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790556.990637}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, RMSE): 38.4718477148\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, mean_absolute_QuantileLoss): 15674.954983281263\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, mean_wQuantileLoss): 0.13022310362450165\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.1]): 0.07294256186523915\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.2]): 0.1149271210025115\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.3]): 0.14330489382542694\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.4]): 0.1606404231779003\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.5]): 0.1677404539462631\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.6]): 0.1650843414085281\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.7]): 0.1494489356583388\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.8]): 0.12050367210006815\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #test_score (algo-1, wQuantileLoss[0.9]): 0.07741552963623861\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.130223103625\u001b[0m\n",
      "\u001b[34m[04/25/2020 04:56:30 INFO 140327940233024] #quality_metric: host=algo-1, test RMSE <loss>=38.4718477148\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 5233047.592163086, \"sum\": 5233047.592163086, \"min\": 5233047.592163086}, \"setuptime\": {\"count\": 1, \"max\": 7.867097854614258, \"sum\": 7.867097854614258, \"min\": 7.867097854614258}}, \"EndTime\": 1587790590.670926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1587790590.397711}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-25 04:56:47 Uploading - Uploading generated training model\n",
      "2020-04-25 04:56:47 Completed - Training job completed\n",
      "Training seconds: 5310\n",
      "Billable seconds: 5310\n"
     ]
    }
   ],
   "source": [
    "# This will takes around 35 minutes to train with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Job Name and Create End Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "# Hard code name for now as we stopped the notebook.  \n",
    "# If you do this in a single sitting, you don't need to hard code\n",
    "# job_name = 'deepar-biketrain-with-categories-2018-12-21-04-05-44-478'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: deepar-biketrain-no-categories-2020-04-25-03-26-34-872\n"
     ]
    }
   ],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-east-2:399426528351:endpoint/deepar-biketrain-no-categories-2020-04-25-03-26-34-872\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-90b1700f136b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# you can delete from sagemaker management console or through command line or throught code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mdelete_endpoint\u001b[0;34m(self, endpoint_name)\u001b[0m\n\u001b[1;32m   2447\u001b[0m         \"\"\"\n\u001b[1;32m   2448\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deleting endpoint with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2449\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_config_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-east-2:399426528351:endpoint/deepar-biketrain-no-categories-2020-04-25-03-26-34-872\"."
     ]
    }
   ],
   "source": [
    "# Don't forget to terminate the end point after completing the demo\n",
    "# Otherwise, you account will accumulate hourly charges\n",
    "\n",
    "# you can delete from sagemaker management console or through command line or throught code\n",
    "\n",
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
